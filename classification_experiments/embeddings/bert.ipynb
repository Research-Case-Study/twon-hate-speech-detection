{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbfe59d6-a540-4f61-a1d9-91c71de9036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MachineLearning\\UniTrier\\RCS\\twon-hate-speech-detection\\.venv\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0768bf-5d37-4de9-bdb5-912dbffc013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = pd.read_csv(\"C:\\\\MachineLearning\\\\UniTrier\\\\RCS\\\\twon-hate-speech-detection\\\\Data\\\\knowledge_base_data\\\\TRAIN_DF.csv\", index_col=1)\n",
    "TEST_DF = pd.read_csv(\"C:\\\\MachineLearning\\\\UniTrier\\\\RCS\\\\twon-hate-speech-detection\\\\Data\\\\knowledge_base_data\\\\TEST_DF.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fec505e-cbe3-4ea8-a5ec-ca0a3c99e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DF = df.loc[~df.index.isin(TEST_DF.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60db208c-c92e-45bb-821f-aa0b2402b6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF['Response'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc68d9cc-704e-49e7-8bee-c43584b13cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DF['tweet_text'] = TRAIN_DF['tweet_text'] + \"\\nContext:\" + TRAIN_DF['Response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3a81e-bc0a-4faa-85aa-d5b6f280c083",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e2c72-f262-47a8-b224-834ae444d1ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Rag for (With explanations case) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf65414-0658-4223-b875-ae8c15f9b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "# import torch\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Device: {device}\")\n",
    "\n",
    "# EMBEDDINGS_MODEL = SentenceTransformer(\n",
    "#     \"dunzhang/stella_en_1.5B_v5\",\n",
    "#     trust_remote_code=True,\n",
    "#     device=device,\n",
    "#     #cache_folder='/media/data/hugging_face_cache'\n",
    "# )\n",
    "\n",
    "# qdrant_client = QdrantClient(\n",
    "#     url=\"https://cf521759-86ad-49b4-b7f7-07fe3bb5f2ec.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
    "#     api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-Uof-NN6Q2IUWexHgY26SBVNHKIiJP32fF2gchKkWgI\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51790bd4-d44b-4e75-b755-1df4e65ea8e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Bert Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365f1711-5277-4d1e-ad07-a9e343626cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        # Convert to lists to ensure indexing works correctly\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "        self.labels = labels.tolist() if hasattr(labels, 'tolist') else list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Verify data integrity\n",
    "        assert len(self.texts) == len(\n",
    "            self.labels), \"Texts and labels must have the same length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.texts):\n",
    "            raise IndexError(\n",
    "                f\"Index {idx} out of bounds for dataset of size {len(self.texts)}\")\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, le, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_steps = 0\n",
    "\n",
    "        #for batch_idx, batch in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Print every 50 steps\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                avg_train_loss = train_loss / train_steps\n",
    "                print(f'Step {batch_idx + 1} - Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                val_steps += 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_val_loss = val_loss / val_steps\n",
    "        accuracy = (np.array(all_predictions) ==\n",
    "                    np.array(all_true_labels)).mean()\n",
    "\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation accuracy: {accuracy:.4f}')\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(all_true_labels, all_predictions,\n",
    "                                    target_names=le.classes_))\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'New best model saved with accuracy: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca535273-b4a1-49d6-b6f6-0c14e4181261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18131, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF.shape # = TRAIN_DF[:18840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218216b3-a05b-40db-acc6-26f23964d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DF.to_csv(\"Data/TRAIN_DF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918cf8cf-a5d1-4951-ae00-09ecef20f89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18131, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ff9300-4116-4c4b-b2b3-3862b6ac670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89383112-1ecb-445c-8487-8335897ac202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "normal        672\n",
       "hatespeech    672\n",
       "offensive     672\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf4222d-557a-4a3c-81f0-a14db12260a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the main function to use TRAIN_DF for training and TEST_DF for testing\n",
    "def main():\n",
    "    try:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load the training and test data\n",
    "        train_df = TRAIN_DF # pd.read_csv('Data/TRAIN_DF.csv')\n",
    "        test_df = TEST_DF # pd.read_csv('Data/TEST_DF.csv')\n",
    "\n",
    "        # Data validation\n",
    "        required_columns = ['tweet_text', 'label']\n",
    "        if not all(col in train_df.columns for col in required_columns):\n",
    "            raise ValueError(f\"Dataset must contain columns: {required_columns}\")\n",
    "     \n",
    "        train_df = train_df.dropna(subset=['tweet_text', 'label'])\n",
    "        test_df = test_df.dropna(subset=['tweet_text', 'label'])\n",
    "\n",
    "        # Reset index after dropping NaN values\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "        # Convert labels to numerical values\n",
    "        le = LabelEncoder()\n",
    "        train_df['label'] = le.fit_transform(train_df['label'])\n",
    "        test_df['label'] = le.transform(test_df['label'])  # Use the same encoder for test data\n",
    "\n",
    "        # Split the dataset into train and validation sets\n",
    "        print(\"\\nSplitting training dataset...\")\n",
    "        \n",
    "        val_size = test_df.shape[0]\n",
    "        test_size = val_size / len(train_df)\n",
    "        \n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_df['tweet_text'],\n",
    "            train_df['label'],\n",
    "            test_size=test_size,\n",
    "            random_state=42,\n",
    "            stratify=train_df['label']\n",
    "        )\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        print(\"Initializing BERT model and tokenizer...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "        num_labels = len(le.classes_)\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'google-bert/bert-large-uncased',\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=40)\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        model.to(device)\n",
    "\n",
    "        # Train the model\n",
    "        train_model(model, train_loader, val_loader, device, le, 4)\n",
    "\n",
    "        # Testing on TEST_DF (used only for testing at the end)\n",
    "        print(\"\\nEvaluating model on the test dataset...\")\n",
    "        test_texts = test_df['tweet_text']\n",
    "        test_labels = test_df['label']\n",
    "\n",
    "        test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=40)\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        test_loss = 0\n",
    "        test_steps = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Testing'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                test_steps += 1\n",
    "\n",
    "        avg_test_loss = test_loss / test_steps\n",
    "        print(f'Average test loss: {avg_test_loss:.4f}')\n",
    "        print('\\nTest Classification Report:')\n",
    "        print(classification_report(all_true_labels, all_predictions, target_names=le.classes_))\n",
    "\n",
    "        # Save the final model and tokenizer\n",
    "        model.save_pretrained('fine_tuned_bert_with_explainations')\n",
    "        tokenizer.save_pretrained('fine_tuned_bert_with_explainations')\n",
    "        np.save('label_classes.npy', le.classes_)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        print(\"\\nFull error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d796c-3513-4da0-b1b7-79c0dfc942b0",
   "metadata": {},
   "source": [
    "##  Perform Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c251c-03ec-4ba7-8c13-4d75e47ec057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Without Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ad0022b-8541-4892-9e28-6fce55b7ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\UniTrier\\RCS\\twon-hate-speech-detection\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "Step 50 - Average training loss: 1.0584\n",
      "Step 100 - Average training loss: 1.0063\n",
      "Step 150 - Average training loss: 0.9653\n",
      "Step 200 - Average training loss: 0.9304\n",
      "Step 250 - Average training loss: 0.9118\n",
      "Step 300 - Average training loss: 0.8939\n",
      "Step 350 - Average training loss: 0.8785\n",
      "Step 400 - Average training loss: 0.8673\n",
      "Average training loss: 0.8664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:11<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7649\n",
      "Validation accuracy: 0.6731\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.79      0.68      0.73       618\n",
      "      normal       0.70      0.77      0.73       832\n",
      "   offensive       0.52      0.52      0.52       566\n",
      "\n",
      "    accuracy                           0.67      2016\n",
      "   macro avg       0.67      0.66      0.66      2016\n",
      "weighted avg       0.68      0.67      0.67      2016\n",
      "\n",
      "New best model saved with accuracy: 0.6731\n",
      "\n",
      "Epoch 2/4\n",
      "Step 50 - Average training loss: 0.7217\n",
      "Step 100 - Average training loss: 0.7261\n",
      "Step 150 - Average training loss: 0.7265\n",
      "Step 200 - Average training loss: 0.7279\n",
      "Step 250 - Average training loss: 0.7305\n",
      "Step 300 - Average training loss: 0.7291\n",
      "Step 350 - Average training loss: 0.7288\n",
      "Step 400 - Average training loss: 0.7287\n",
      "Average training loss: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:11<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7264\n",
      "Validation accuracy: 0.6880\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.72      0.78      0.75       618\n",
      "      normal       0.72      0.78      0.75       832\n",
      "   offensive       0.56      0.45      0.50       566\n",
      "\n",
      "    accuracy                           0.69      2016\n",
      "   macro avg       0.67      0.67      0.67      2016\n",
      "weighted avg       0.68      0.69      0.68      2016\n",
      "\n",
      "New best model saved with accuracy: 0.6880\n",
      "\n",
      "Epoch 3/4\n",
      "Step 50 - Average training loss: 0.6205\n",
      "Step 100 - Average training loss: 0.6161\n",
      "Step 150 - Average training loss: 0.6129\n",
      "Step 200 - Average training loss: 0.6091\n",
      "Step 250 - Average training loss: 0.6175\n",
      "Step 300 - Average training loss: 0.6186\n",
      "Step 350 - Average training loss: 0.6242\n",
      "Step 400 - Average training loss: 0.6288\n",
      "Average training loss: 0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:11<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7480\n",
      "Validation accuracy: 0.6875\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.69      0.81      0.74       618\n",
      "      normal       0.74      0.78      0.76       832\n",
      "   offensive       0.57      0.43      0.49       566\n",
      "\n",
      "    accuracy                           0.69      2016\n",
      "   macro avg       0.67      0.67      0.66      2016\n",
      "weighted avg       0.68      0.69      0.68      2016\n",
      "\n",
      "\n",
      "Epoch 4/4\n",
      "Step 50 - Average training loss: 0.5059\n",
      "Step 100 - Average training loss: 0.5050\n",
      "Step 150 - Average training loss: 0.5162\n",
      "Step 200 - Average training loss: 0.5126\n",
      "Step 250 - Average training loss: 0.5170\n",
      "Step 300 - Average training loss: 0.5193\n",
      "Step 350 - Average training loss: 0.5199\n",
      "Step 400 - Average training loss: 0.5202\n",
      "Average training loss: 0.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:11<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.8402\n",
      "Validation accuracy: 0.6756\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.80      0.70      0.75       618\n",
      "      normal       0.71      0.73      0.72       832\n",
      "   offensive       0.52      0.56      0.54       566\n",
      "\n",
      "    accuracy                           0.68      2016\n",
      "   macro avg       0.68      0.67      0.67      2016\n",
      "weighted avg       0.68      0.68      0.68      2016\n",
      "\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:11<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.8737\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.77      0.70      0.73       672\n",
      "      normal       0.63      0.76      0.69       672\n",
      "   offensive       0.61      0.54      0.57       672\n",
      "\n",
      "    accuracy                           0.67      2016\n",
      "   macro avg       0.67      0.67      0.66      2016\n",
      "weighted avg       0.67      0.67      0.66      2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without explanations\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140cc5e-cb93-4feb-9b0e-b8f0b288b285",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### With Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f126c695-1097-4ff3-903b-aaf82248fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\UniTrier\\RCS\\twon-hate-speech-detection\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "Step 50 - Average training loss: 0.9888\n",
      "Step 100 - Average training loss: 0.8318\n",
      "Step 150 - Average training loss: 0.7487\n",
      "Step 200 - Average training loss: 0.6898\n",
      "Step 250 - Average training loss: 0.6364\n",
      "Step 300 - Average training loss: 0.5976\n",
      "Step 350 - Average training loss: 0.5676\n",
      "Step 400 - Average training loss: 0.5415\n",
      "Average training loss: 0.5406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:16<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.3146\n",
      "Validation accuracy: 0.8909\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.87      0.92      0.89       618\n",
      "      normal       0.87      0.93      0.90       832\n",
      "   offensive       0.95      0.80      0.87       566\n",
      "\n",
      "    accuracy                           0.89      2016\n",
      "   macro avg       0.90      0.88      0.89      2016\n",
      "weighted avg       0.89      0.89      0.89      2016\n",
      "\n",
      "New best model saved with accuracy: 0.8909\n",
      "\n",
      "Epoch 2/4\n",
      "Step 50 - Average training loss: 0.2984\n",
      "Step 100 - Average training loss: 0.3026\n",
      "Step 150 - Average training loss: 0.3100\n",
      "Step 200 - Average training loss: 0.3111\n",
      "Step 250 - Average training loss: 0.3087\n",
      "Step 300 - Average training loss: 0.3030\n",
      "Step 350 - Average training loss: 0.3016\n",
      "Step 400 - Average training loss: 0.2989\n",
      "Average training loss: 0.2996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:16<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.2934\n",
      "Validation accuracy: 0.8914\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.87      0.92      0.89       618\n",
      "      normal       0.88      0.93      0.90       832\n",
      "   offensive       0.94      0.81      0.87       566\n",
      "\n",
      "    accuracy                           0.89      2016\n",
      "   macro avg       0.90      0.89      0.89      2016\n",
      "weighted avg       0.89      0.89      0.89      2016\n",
      "\n",
      "New best model saved with accuracy: 0.8914\n",
      "\n",
      "Epoch 3/4\n",
      "Step 50 - Average training loss: 0.2280\n",
      "Step 100 - Average training loss: 0.2391\n",
      "Step 150 - Average training loss: 0.2349\n",
      "Step 200 - Average training loss: 0.2303\n",
      "Step 250 - Average training loss: 0.2325\n",
      "Step 300 - Average training loss: 0.2338\n",
      "Step 350 - Average training loss: 0.2332\n",
      "Step 400 - Average training loss: 0.2333\n",
      "Average training loss: 0.2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:15<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.3072\n",
      "Validation accuracy: 0.8938\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.84      0.96      0.89       618\n",
      "      normal       0.93      0.88      0.91       832\n",
      "   offensive       0.91      0.84      0.87       566\n",
      "\n",
      "    accuracy                           0.89      2016\n",
      "   macro avg       0.89      0.89      0.89      2016\n",
      "weighted avg       0.90      0.89      0.89      2016\n",
      "\n",
      "New best model saved with accuracy: 0.8938\n",
      "\n",
      "Epoch 4/4\n",
      "Step 50 - Average training loss: 0.1627\n",
      "Step 100 - Average training loss: 0.1639\n",
      "Step 150 - Average training loss: 0.1639\n",
      "Step 200 - Average training loss: 0.1682\n",
      "Step 250 - Average training loss: 0.1666\n",
      "Step 300 - Average training loss: 0.1717\n",
      "Step 350 - Average training loss: 0.1715\n",
      "Step 400 - Average training loss: 0.1707\n",
      "Average training loss: 0.1707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:16<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.3194\n",
      "Validation accuracy: 0.9053\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.88      0.94      0.91       618\n",
      "      normal       0.92      0.92      0.92       832\n",
      "   offensive       0.92      0.84      0.88       566\n",
      "\n",
      "    accuracy                           0.91      2016\n",
      "   macro avg       0.91      0.90      0.90      2016\n",
      "weighted avg       0.91      0.91      0.90      2016\n",
      "\n",
      "New best model saved with accuracy: 0.9053\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:11<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.1769\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.62      0.82      0.71       672\n",
      "      normal       0.55      0.83      0.66       672\n",
      "   offensive       0.81      0.14      0.24       672\n",
      "\n",
      "    accuracy                           0.60      2016\n",
      "   macro avg       0.66      0.60      0.53      2016\n",
      "weighted avg       0.66      0.60      0.53      2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with explanations\n",
    "TRAIN_DF['tweet_text'] = TRAIN_DF['tweet_text'] + \"\\nContext:\" + TRAIN_DF['Response']\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
