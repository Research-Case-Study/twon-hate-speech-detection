{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123674ed-6a59-47ed-a1c7-66fd654021d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Change to the parent directory of the notebook\n",
    "\n",
    "os.chdir('..')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebad1aa-dc6a-499b-8e16-34856436f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea1ce53-cde1-4170-8e00-15988d542614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b04916-93e7-40cd-8d8c-2cefaccb613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Data/knowledge_base_data/explainations_CoT_Hermes_partial.csv\") #, index_col=0) # first 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00b732b-c821-40b0-ba94-69d808eed2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Data/knowledge_base_data/explainations_CoT_Hermes_partial_2.csv\", index_col=1) #9000-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c4218c-8389-4e4d-97e7-b5ed22004681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"Data/knowledge_base_data/explainations_CoT_Hermes_partial_3.csv\", index_col=1) # 15k to 19k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8731fb0-8f2a-4f20-ab6f-7cb3b0b132ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"Data/knowledge_base_data/explainations_CoT_Hermes_partial_4.csv\", index_col=1) # 11k to 15k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d33f99-100f-4235-aad3-a27f009575dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8a3a49-e5f7-4da3-9bf5-390cd8fb9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df1, df2, df3, df4], ignore_index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "230147c0-19cd-43b7-b5e4-4e6d56f81436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['African', 'Arab', 'Asexual', 'Asian', 'Bisexual', 'Buddhism', 'Caucasian', 'Christian', 'Disability', 'Economic', 'Heterosexual', 'Hindu', 'Hispanic', 'Homosexual', 'Indian', 'Indigenous', 'Islam', 'Jewish', 'Men', 'Minority', 'None', 'Nonreligious', 'Other', 'Refugee', 'Women']\n"
     ]
    }
   ],
   "source": [
    "# Extract unique words from the 'target' column\n",
    "unique_words = set()\n",
    "\n",
    "df_combined['target'].dropna().apply(lambda x: unique_words.update(x.split(', ')))\n",
    "\n",
    "# Convert to a sorted list (optional)\n",
    "unique_words_list = sorted(unique_words)\n",
    "\n",
    "# Print or return the list\n",
    "print(unique_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee358cf7-d223-4f72-bf6c-5b0976fedd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the multi-category entries and getting unique words\n",
    "unique_words = set()\n",
    "df_combined['target'].dropna().apply(lambda x: unique_words.update(x.split(', ')))\n",
    "\n",
    "# Count of unique words\n",
    "len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b2cc0b5-e5f0-49da-99c4-8402c4d5a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cc1fd-470c-4f30-9c8f-0d0429c7dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv(\"Data/dataset.csv\")\n",
    "# df = df[['tweet_text', 'label']]\n",
    "\n",
    "# # Assuming your label column is called 'label'\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['labels'] = label_encoder.fit_transform(df['label'])\n",
    "# df = df[['tweet_text', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30628d99-ffc2-44b9-a70d-139c3df980fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Split dataset\n",
    "# train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# # Convert to Hugging Face Dataset\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# # Initialize tokenizer and model\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# # Tokenize data\n",
    "# # Tokenize data with max_length\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['tweet_text'], \n",
    "#         padding='max_length',  # Pad to the max length (you can specify a max_length if needed)\n",
    "#         truncation=True,        # Truncate if the sequence is too long\n",
    "#         max_length=128          # Set max_length as per your requirement\n",
    "#     )\n",
    "\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Rename the 'label' column to 'labels' to match the expected column name\n",
    "# # train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "# # val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# # Set the device to GPU 1 manually (cuda:1)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "# model.to(device)  # Move model to the selected device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f002c29-a164-4f1c-a264-25fba54cbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     acc = accuracy_score(labels, predictions)\n",
    "#     f1 = f1_score(labels, predictions, average='weighted')\n",
    "#     return {\"accuracy\": acc, \"f1\": f1}\n",
    "    \n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',  # Directory for final model\n",
    "#     evaluation_strategy=\"steps\",  # Evaluate at end of each epoch\n",
    "#     eval_steps=50,\n",
    "#     per_device_train_batch_size=24,\n",
    "#     per_device_eval_batch_size=24,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_dir='./logs',  # Directory for logs\n",
    "#     logging_strategy=\"steps\",  # Log training progress\n",
    "#     logging_steps=10,  # Log every 10 steps\n",
    "#     save_strategy=\"no\",  # Disable automatic checkpoint saving\n",
    "#     save_total_limit=0,  # Ensure no checkpoints are kept\n",
    "#     no_cuda=False,  # Use GPU if available,\n",
    "#     # learning_rate=2e-5,\n",
    "#     # max_grad_norm=1.0,\n",
    "#     # fp16=False,\n",
    "#     # report_to=[\"console\"]\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # debug=True  # Enable debugging\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the final model\n",
    "# trainer.save_model(\"./final_model\")  # Saves only at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "483485eb-4db6-42a5-bdcd-a4cd0b60fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset['labels'][:10])  # Check if they look correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94bc8a23-adb1-4935-95e1-00c53fa9829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0])\n",
    "# print(val_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "317bfd4f-4d97-412a-ada1-6268a7580f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Get predictions\n",
    "# predictions = trainer.predict(val_dataset)\n",
    "# y_pred = predictions.predictions.argmax(axis=-1)\n",
    "# y_true = val_df['label'].values  # True labels\n",
    "\n",
    "# # Print classification report\n",
    "# print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae6009c-7bf9-4852-899f-e82ca4c6aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b873c02d3c74a60bbbf3a3bde6792e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_49972\\935123486.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='831' max='2268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 831/2268 06:57 < 12:03, 1.99 it/s, Epoch 1.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>0.929643</td>\n",
       "      <td>0.582630</td>\n",
       "      <td>0.539938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>0.888768</td>\n",
       "      <td>0.605955</td>\n",
       "      <td>0.591407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.827797</td>\n",
       "      <td>0.650124</td>\n",
       "      <td>0.641985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.844200</td>\n",
       "      <td>0.832451</td>\n",
       "      <td>0.620347</td>\n",
       "      <td>0.588882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.839361</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.620586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.798361</td>\n",
       "      <td>0.655087</td>\n",
       "      <td>0.629946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.720600</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.666005</td>\n",
       "      <td>0.655984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.790300</td>\n",
       "      <td>0.782104</td>\n",
       "      <td>0.671464</td>\n",
       "      <td>0.672194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.818900</td>\n",
       "      <td>0.779998</td>\n",
       "      <td>0.672457</td>\n",
       "      <td>0.673772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.775700</td>\n",
       "      <td>0.767702</td>\n",
       "      <td>0.673945</td>\n",
       "      <td>0.668762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.146863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.146863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.146863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.146863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.146863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.146863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m\n\u001b[0;32m     71\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     72\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     73\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorWithPadding(tokenizer)\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[0;32m     85\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2497\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2493\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated loss must be on the original device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but device in use is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss_step\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2494\u001b[0m         )\n\u001b[0;32m   2495\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss_step\n\u001b[1;32m-> 2497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_flos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sync_step:\n\u001b[0;32m   2500\u001b[0m     \u001b[38;5;66;03m# Since we perform prefetching, we need to manually set sync_gradients to True\u001b[39;00m\n\u001b[0;32m   2501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39m_set_sync_gradients(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4413\u001b[0m, in \u001b[0;36mTrainer.floating_point_ops\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4409\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   4411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, logits, labels)\n\u001b[1;32m-> 4413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloating_point_ops\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]):\n\u001b[0;32m   4414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4415\u001b[0m \u001b[38;5;124;03m    For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[0;32m   4416\u001b[0m \u001b[38;5;124;03m    operations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4424\u001b[0m \u001b[38;5;124;03m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[0;32m   4425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating_point_ops\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Data/dataset.csv\")\n",
    "df = df[['tweet_text', 'label']]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['labels'] = label_encoder.fit_transform(df['label'])\n",
    "df = df[['tweet_text', 'labels']]\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['tweet_text'].tolist(), df['labels'].tolist(), test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Convert data into Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": train_texts + val_texts,\n",
    "    \"label\": train_labels + val_labels\n",
    "})\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = dataset.select(range(len(train_texts)))\n",
    "val_dataset = dataset.select(range(len(train_texts), len(train_texts) + len(val_texts)))\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    no_cuda=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(\"./final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d55ee0-a5b4-4270-86e9-38ed02c23738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict_values' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtrain_test_split(train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Map the dataset to tensors\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(prepare_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mmap(prepare_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict_values' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', language='english')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Prepare the dataset for BERT\n",
    "def prepare_data(examples):\n",
    "    encodings = tokenizer(\n",
    "        examples['tweet_text'], \n",
    "        truncation=True, \n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = torch.tensor([label_encoder.transform(example) for example in examples['label']], dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'].flatten(),\n",
    "        'attention_mask': encodings['attention_mask'].flatten(),\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create dataset from dataframe\n",
    "dataset = Dataset.from_dict(df.to_dict())\n",
    "train_dataset = dataset.train_test_split(train_size=0.8, shuffle=True).values()\n",
    "\n",
    "# Map the dataset to tensors\n",
    "train_dataset = train_dataset.map(prepare_data, batched=True)\n",
    "val_dataset = val_dataset.map(prepare_data, batched=True)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55179f8-acdb-4f1d-9168-e92ff78ce9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6aedf3-800b-45dd-be7b-8f7460cc4f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce372a5-1f81-41e8-b70d-93117ba0090b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7eb14-026d-4fa4-88b8-7c8a5921f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Data/dataset.csv\")\n",
    "df = df[['tweet_text', 'label']]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['labels'] = label_encoder.fit_transform(df['label'])\n",
    "df = df[['tweet_text', 'labels']]\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['tweet_text'].tolist(), df['labels'].tolist(), test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94693fb-01c3-4d4b-b8a6-e94f0ebe5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872343d-36d2-4dc4-85c1-e4af7e5e1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533eff2-2268-4113-a92f-4eb095978eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32229a-d35f-418a-ad5d-d100a40b60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fee75c38-a51b-49e7-9c2c-e50944e117a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"Data/dataset.csv\")\n",
    "\n",
    "split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0cdc40-580c-4cd5-bd5c-a175100bcdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'post_id', 'tweet_text', 'key_features', 'target', 'label', 'annotator_1_label', 'annotator_1_target', 'annotator_2_label', 'annotator_2_target', 'annotator_3_label', 'annotator_3_target'],\n",
       "        num_rows: 20148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80145168-cbd3-45d7-ab20-0f778129a7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a26ab40dab4d529e1e986e188d90e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1ab86b04994f27b72c85e24d68e45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2015 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"tweet_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = split_datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98ea7b87-2d9e-47a0-86cb-746fd2987db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c95759c-1ed5-4c21-b300-1907244ca943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5b29302-39de-4cbb-abfb-504a5f292959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\",\n",
    "    num_labels=len(set(dataset[\"train\"][\"label\"]))  # Adjust based on your labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4db4b57-ae23-4d2b-9fc8-6c53d5630f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ed295-6e2f-406c-87fb-ee6c0ae959e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f4f7cf5-19f0-4bd0-881e-b47e2a2e765a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     14\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\accelerate\\data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mC:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:141\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    139\u001b[0m     label \u001b[38;5;241m=\u001b[39m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    140\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[1;32m--> 141\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m first \u001b[38;5;129;01mand\u001b[39;00m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac834f35-439d-4faf-a05b-53f91e018449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ccf8e-3e13-4d09-ba60-b0d3e65c9587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f93f07-a897-4857-8113-1cf637f7d0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec8e9c-4ff3-4aa9-a67d-3d95cc756dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23fa4b40-b0df-41ad-a4e8-e19234bb1c78",
   "metadata": {},
   "source": [
    "## bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afcc2657-6be4-4662-aac3-b954087629ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Input</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>key_features</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>Response</th>\n",
       "      <th>Time Taken (s)</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>{'post_id': '1179055004553900032_twitter', 'tw...</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The text appears to be a personal statement ab...</td>\n",
       "      <td>39.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>{'post_id': '1179063826874032128_twitter', 'tw...</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The text falls under the category of normal. H...</td>\n",
       "      <td>18.88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>{'post_id': '1178793830532956161_twitter', 'tw...</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>African, None</td>\n",
       "      <td>This text cannot be classified as normal. The ...</td>\n",
       "      <td>13.28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>{'post_id': '1179088797964763136_twitter', 'tw...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>['ching', 'chong']</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Asian</td>\n",
       "      <td>The text is classified as hate speech due to t...</td>\n",
       "      <td>27.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>{'post_id': '1179085312976445440_twitter', 'tw...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>['bitch', 'i', 'hate', 'white', 'bitches']</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Caucasian, Women</td>\n",
       "      <td>**Hate Speech**:\\r\\n\\r\\nThe tweet explicitly e...</td>\n",
       "      <td>18.15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{'post_id': '18011618_gab', 'tweet_text': 'mac...</td>\n",
       "      <td>macron is going to demand that we take scumbag...</td>\n",
       "      <td>['scumbag', 'refugees']</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Refugee</td>\n",
       "      <td>The text is classified as offensive due to the...</td>\n",
       "      <td>24.93</td>\n",
       "      <td>995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{'post_id': '18012075_gab', 'tweet_text': 'the...</td>\n",
       "      <td>the national health serivces of the uk making ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>Men, None, Women</td>\n",
       "      <td>The text falls into the neutral category due t...</td>\n",
       "      <td>17.51</td>\n",
       "      <td>996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{'post_id': '18013591_gab', 'tweet_text': 'and...</td>\n",
       "      <td>and obama hung out with white ppl in martha vi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>African, Caucasian, None</td>\n",
       "      <td>The text falls into the normal category due to...</td>\n",
       "      <td>32.13</td>\n",
       "      <td>997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{'post_id': '18015405_gab', 'tweet_text': 'it ...</td>\n",
       "      <td>it is the white man fault the nigger raped me ...</td>\n",
       "      <td>['white', 'man', 'fault', 'the', 'nigger', 'ra...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>African, Hispanic, None, Women</td>\n",
       "      <td>The tweet text is classified as hate speech du...</td>\n",
       "      <td>25.79</td>\n",
       "      <td>998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{'post_id': '18016424_gab', 'tweet_text': 'wro...</td>\n",
       "      <td>wrong for not throwing the arabs out earlier l...</td>\n",
       "      <td>['wrong', 'for', 'not', 'throwing', 'the', 'ar...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Arab</td>\n",
       "      <td>Offensive:\\n\\nThe tweet text contains the phra...</td>\n",
       "      <td>30.11</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Row Number                                              Input  \\\n",
       "0             1.0  {'post_id': '1179055004553900032_twitter', 'tw...   \n",
       "1             2.0  {'post_id': '1179063826874032128_twitter', 'tw...   \n",
       "2             3.0  {'post_id': '1178793830532956161_twitter', 'tw...   \n",
       "3             4.0  {'post_id': '1179088797964763136_twitter', 'tw...   \n",
       "4             5.0  {'post_id': '1179085312976445440_twitter', 'tw...   \n",
       "...           ...                                                ...   \n",
       "15995         NaN  {'post_id': '18011618_gab', 'tweet_text': 'mac...   \n",
       "15996         NaN  {'post_id': '18012075_gab', 'tweet_text': 'the...   \n",
       "15997         NaN  {'post_id': '18013591_gab', 'tweet_text': 'and...   \n",
       "15998         NaN  {'post_id': '18015405_gab', 'tweet_text': 'it ...   \n",
       "15999         NaN  {'post_id': '18016424_gab', 'tweet_text': 'wro...   \n",
       "\n",
       "                                              tweet_text  \\\n",
       "0      i dont think im getting my baby them white 9 h...   \n",
       "1      we cannot continue calling ourselves feminists...   \n",
       "2                          nawt yall niggers ignoring me   \n",
       "3      <user> i am bit confused coz chinese ppl can n...   \n",
       "4      this bitch in whataburger eating a burger with...   \n",
       "...                                                  ...   \n",
       "15995  macron is going to demand that we take scumbag...   \n",
       "15996  the national health serivces of the uk making ...   \n",
       "15997  and obama hung out with white ppl in martha vi...   \n",
       "15998  it is the white man fault the nigger raped me ...   \n",
       "15999  wrong for not throwing the arabs out earlier l...   \n",
       "\n",
       "                                            key_features       label  \\\n",
       "0                                                     []      normal   \n",
       "1                                                     []      normal   \n",
       "2                                                     []      normal   \n",
       "3                                     ['ching', 'chong']  hatespeech   \n",
       "4             ['bitch', 'i', 'hate', 'white', 'bitches']  hatespeech   \n",
       "...                                                  ...         ...   \n",
       "15995                            ['scumbag', 'refugees']   offensive   \n",
       "15996                                                 []      normal   \n",
       "15997                                                 []      normal   \n",
       "15998  ['white', 'man', 'fault', 'the', 'nigger', 'ra...  hatespeech   \n",
       "15999  ['wrong', 'for', 'not', 'throwing', 'the', 'ar...   offensive   \n",
       "\n",
       "                               target  \\\n",
       "0                                 NaN   \n",
       "1                                 NaN   \n",
       "2                       African, None   \n",
       "3                               Asian   \n",
       "4                    Caucasian, Women   \n",
       "...                               ...   \n",
       "15995                         Refugee   \n",
       "15996                Men, None, Women   \n",
       "15997        African, Caucasian, None   \n",
       "15998  African, Hispanic, None, Women   \n",
       "15999                            Arab   \n",
       "\n",
       "                                                Response  Time Taken (s)  \\\n",
       "0      The text appears to be a personal statement ab...           39.40   \n",
       "1      The text falls under the category of normal. H...           18.88   \n",
       "2      This text cannot be classified as normal. The ...           13.28   \n",
       "3      The text is classified as hate speech due to t...           27.10   \n",
       "4      **Hate Speech**:\\r\\n\\r\\nThe tweet explicitly e...           18.15   \n",
       "...                                                  ...             ...   \n",
       "15995  The text is classified as offensive due to the...           24.93   \n",
       "15996  The text falls into the neutral category due t...           17.51   \n",
       "15997  The text falls into the normal category due to...           32.13   \n",
       "15998  The tweet text is classified as hate speech du...           25.79   \n",
       "15999  Offensive:\\n\\nThe tweet text contains the phra...           30.11   \n",
       "\n",
       "       Unnamed: 0  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "15995       995.0  \n",
       "15996       996.0  \n",
       "15997       997.0  \n",
       "15998       998.0  \n",
       "15999       999.0  \n",
       "\n",
       "[10000 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca0768bf-5d37-4de9-bdb5-912dbffc013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97b1cf7b-0600-4b5c-a1f5-640f20fa0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DF = pd.read_csv(\"Data/TRAIN_DF.csv\", index_col=0)\n",
    "TEST_DF = pd.read_csv(\"Data/TEST_DF.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fec505e-cbe3-4ea8-a5ec-ca0a3c99e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = df.loc[~df.index.isin(TEST_DF.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fdadb79-e7f6-4e8a-9b3e-cf56cd18bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from exp datasets\n",
    "TEST_DF = pd.read_csv(\"Data/TEST_DF.csv\", index_col=0)\n",
    "TRAIN_DF = df_combined\n",
    "TRAIN_DF = TRAIN_DF.loc[~TRAIN_DF.index.isin(TEST_DF.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342fbd5-616a-467d-b602-6a78b27d3936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60db208c-c92e-45bb-821f-aa0b2402b6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF['Response'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc68d9cc-704e-49e7-8bee-c43584b13cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF['tweet_text'] = TRAIN_DF['tweet_text'] + \"\\nContext:\" + TRAIN_DF['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365f1711-5277-4d1e-ad07-a9e343626cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        # Convert to lists to ensure indexing works correctly\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "        self.labels = labels.tolist() if hasattr(labels, 'tolist') else list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Verify data integrity\n",
    "        assert len(self.texts) == len(\n",
    "            self.labels), \"Texts and labels must have the same length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.texts):\n",
    "            raise IndexError(\n",
    "                f\"Index {idx} out of bounds for dataset of size {len(self.texts)}\")\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, le, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_steps = 0\n",
    "\n",
    "        #for batch_idx, batch in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Print every 50 steps\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                avg_train_loss = train_loss / train_steps\n",
    "                print(f'Step {batch_idx + 1} - Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                val_steps += 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_val_loss = val_loss / val_steps\n",
    "        accuracy = (np.array(all_predictions) ==\n",
    "                    np.array(all_true_labels)).mean()\n",
    "\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation accuracy: {accuracy:.4f}')\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(all_true_labels, all_predictions,\n",
    "                                    target_names=le.classes_))\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'New best model saved with accuracy: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca535273-b4a1-49d6-b6f6-0c14e4181261",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = TRAIN_DF[:18840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "218216b3-a05b-40db-acc6-26f23964d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DF.to_csv(\"Data/TRAIN_DF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "918cf8cf-a5d1-4951-ae00-09ecef20f89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17117, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ff9300-4116-4c4b-b2b3-3862b6ac670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1884, 17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89383112-1ecb-445c-8487-8335897ac202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "hatespeech    628\n",
       "normal        628\n",
       "offensive     628\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcf4222d-557a-4a3c-81f0-a14db12260a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the main function to use TRAIN_DF for training and TEST_DF for testing\n",
    "def main():\n",
    "    try:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load the training and test data\n",
    "        train_df = TRAIN_DF # pd.read_csv('Data/TRAIN_DF.csv')\n",
    "        test_df = TEST_DF # pd.read_csv('Data/TEST_DF.csv')\n",
    "\n",
    "        # Data validation\n",
    "        required_columns = ['tweet_text', 'label']\n",
    "        if not all(col in train_df.columns for col in required_columns):\n",
    "            raise ValueError(f\"Dataset must contain columns: {required_columns}\")\n",
    "     \n",
    "        train_df = train_df.dropna(subset=['tweet_text', 'label'])\n",
    "        test_df = test_df.dropna(subset=['tweet_text', 'label'])\n",
    "\n",
    "        # Reset index after dropping NaN values\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "        # Convert labels to numerical values\n",
    "        le = LabelEncoder()\n",
    "        train_df['label'] = le.fit_transform(train_df['label'])\n",
    "        test_df['label'] = le.transform(test_df['label'])  # Use the same encoder for test data\n",
    "\n",
    "        # Split the dataset into train and validation sets\n",
    "        print(\"\\nSplitting training dataset...\")\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_df['tweet_text'],\n",
    "            train_df['label'],\n",
    "            test_size=0.15,\n",
    "            random_state=42,\n",
    "            stratify=train_df['label']\n",
    "        )\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        print(\"Initializing BERT model and tokenizer...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "        num_labels = len(le.classes_)\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'google-bert/bert-large-uncased',\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        model.to(device)\n",
    "\n",
    "        # Train the model\n",
    "        train_model(model, train_loader, val_loader, device, le, 4)\n",
    "\n",
    "        # Testing on TEST_DF (used only for testing at the end)\n",
    "        print(\"\\nEvaluating model on the test dataset...\")\n",
    "        test_texts = test_df['tweet_text']\n",
    "        test_labels = test_df['label']\n",
    "\n",
    "        test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        test_loss = 0\n",
    "        test_steps = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Testing'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                test_steps += 1\n",
    "\n",
    "        avg_test_loss = test_loss / test_steps\n",
    "        print(f'Average test loss: {avg_test_loss:.4f}')\n",
    "        print('\\nTest Classification Report:')\n",
    "        print(classification_report(all_true_labels, all_predictions, target_names=le.classes_))\n",
    "\n",
    "        # Save the final model and tokenizer\n",
    "        model.save_pretrained('fine_tuned_bert_with_explainations')\n",
    "        tokenizer.save_pretrained('fine_tuned_bert_with_explainations')\n",
    "        np.save('label_classes.npy', le.classes_)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        print(\"\\nFull error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ddfe8f8-6dde-4079-920d-133bcf1c2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     try:\n",
    "        \n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#         # Use TRAIN_DF and TEST_DF instead of reading from the file\n",
    "#         train_df = TRAIN_DF\n",
    "#         test_df = TEST_DF\n",
    "\n",
    "#         # Data validation\n",
    "#         required_columns = ['tweet_text', 'label']\n",
    "#         if not all(col in train_df.columns for col in required_columns):\n",
    "#             raise ValueError(f\"Dataset must contain columns: {required_columns}\")\n",
    "\n",
    "#         train_df = train_df.dropna(subset=['tweet_text', 'label'])\n",
    "#         test_df = test_df.dropna(subset=['tweet_text', 'label'])\n",
    "\n",
    "#         # Reset index after dropping NaN values\n",
    "#         train_df = train_df.reset_index(drop=True)\n",
    "#         test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "#         # Convert labels to numerical values\n",
    "#         le = LabelEncoder()\n",
    "#         train_df['label'] = le.fit_transform(train_df['label'])\n",
    "#         test_df['label'] = le.transform(test_df['label'])  # Use the same encoder for test data\n",
    "\n",
    "#         # Print class distribution\n",
    "#         print(\"\\nTraining set class distribution:\")\n",
    "#         for i, label in enumerate(le.classes_):\n",
    "#             count = len(train_df[train_df['label'] == i])\n",
    "#             print(f\"{label}: {count} samples\")\n",
    "\n",
    "#         # Create datasets\n",
    "#         print(\"\\nCreating datasets...\")\n",
    "#         train_texts, train_labels = train_df['tweet_text'], train_df['label']\n",
    "#         test_texts, test_labels = test_df['tweet_text'], test_df['label']\n",
    "\n",
    "#         # Initialize tokenizer and model\n",
    "#         print(\"Initializing BERT model and tokenizer...\")\n",
    "#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         num_labels = len(le.classes_)\n",
    "#         model = BertForSequenceClassification.from_pretrained(\n",
    "#             'bert-base-uncased',\n",
    "#             num_labels=num_labels\n",
    "#         )\n",
    "\n",
    "#         # Create datasets\n",
    "#         train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "#         test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "#         # Create dataloaders\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=50)\n",
    "\n",
    "#         # Move model to GPU if available\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Train the model\n",
    "#         train_model(model, train_loader, test_loader, device, le)\n",
    "\n",
    "#         # Save the final model and tokenizer\n",
    "#         model.save_pretrained('fine_tuned_bert')\n",
    "#         tokenizer.save_pretrained('fine_tuned_bert')\n",
    "#         np.save('label_classes.npy', le.classes_)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError occurred: {str(e)}\")\n",
    "#         print(\"\\nFull error details:\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3a56dbd-3bed-439e-ac30-b3de8893aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     try:\n",
    "        \n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#         df = pd.read_csv('Data/dataset.csv')\n",
    "\n",
    "#         # Data validation\n",
    "#         required_columns = ['tweet_text', 'label']\n",
    "#         if not all(col in df.columns for col in required_columns):\n",
    "#             raise ValueError(\n",
    "#                 f\"Dataset must contain columns: {required_columns}\")\n",
    "     \n",
    "#         df = df.dropna(subset=['tweet_text', 'label'])\n",
    "\n",
    "#         # Reset index after dropping NaN values\n",
    "#         df = df.reset_index(drop=True)\n",
    "\n",
    "#         # Convert labels to numerical values\n",
    "#         le = LabelEncoder()\n",
    "#         df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "#         # Print class distribution\n",
    "       \n",
    "#         for i, label in enumerate(le.classes_):\n",
    "#             count = len(df[df['label'] == i])\n",
    "#             print(f\"{label}: {count} samples\")\n",
    "\n",
    "#         # Split the dataset\n",
    "#         print(\"\\nSplitting dataset...\")\n",
    "#         train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#             df['tweet_text'],\n",
    "#             df['label'],\n",
    "#             test_size=0.2,\n",
    "#             random_state=42,\n",
    "#             stratify=df['label']\n",
    "#         )\n",
    "\n",
    "#         # Initialize tokenizer and model\n",
    "#         print(\"Initializing BERT model and tokenizer...\")\n",
    "#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         num_labels = 3\n",
    "#         model = BertForSequenceClassification.from_pretrained(\n",
    "#             'bert-base-uncased',\n",
    "#             num_labels=num_labels\n",
    "#         )\n",
    "\n",
    "#         # Create datasets\n",
    "#         train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "#         val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "#         # Create dataloaders\n",
    "#         train_loader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=24,\n",
    "#             shuffle=True\n",
    "#         )\n",
    "#         val_loader = DataLoader(\n",
    "#             val_dataset,\n",
    "#             batch_size=24\n",
    "#         )\n",
    "\n",
    "#         # Move model to GPU if available\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Train the model\n",
    "\n",
    "#         train_model(model, train_loader, val_loader, device, le)\n",
    "\n",
    "#         # Save the final model and tokenizer\n",
    "\n",
    "#         model.save_pretrained('fine_tuned_bert')\n",
    "#         tokenizer.save_pretrained('fine_tuned_bert')\n",
    "#         np.save('label_classes.npy', le.classes_)\n",
    "\n",
    "   \n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError occurred: {str(e)}\")\n",
    "#         print(\"\\nFull error details:\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ad0022b-8541-4892-9e28-6fce55b7ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Python\\py312env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "Step 50 - Average training loss: 0.4917\n",
      "Step 100 - Average training loss: 0.3156\n",
      "Step 150 - Average training loss: 0.2497\n",
      "Step 200 - Average training loss: 0.2107\n",
      "Average training loss: 0.1954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:19<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.0790\n",
      "Validation accuracy: 0.9778\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.97      0.98      0.98       794\n",
      "      normal       0.99      0.97      0.98      1076\n",
      "   offensive       0.98      0.98      0.98       698\n",
      "\n",
      "    accuracy                           0.98      2568\n",
      "   macro avg       0.98      0.98      0.98      2568\n",
      "weighted avg       0.98      0.98      0.98      2568\n",
      "\n",
      "New best model saved with accuracy: 0.9778\n",
      "\n",
      "Epoch 2/4\n",
      "Step 50 - Average training loss: 0.0741\n",
      "Step 100 - Average training loss: 0.0686\n",
      "Step 150 - Average training loss: 0.0652\n",
      "Step 200 - Average training loss: 0.0691\n",
      "Average training loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:19<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.0649\n",
      "Validation accuracy: 0.9821\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.96      1.00      0.98       794\n",
      "      normal       1.00      0.97      0.98      1076\n",
      "   offensive       0.98      0.99      0.98       698\n",
      "\n",
      "    accuracy                           0.98      2568\n",
      "   macro avg       0.98      0.98      0.98      2568\n",
      "weighted avg       0.98      0.98      0.98      2568\n",
      "\n",
      "New best model saved with accuracy: 0.9821\n",
      "\n",
      "Epoch 3/4\n",
      "Step 50 - Average training loss: 0.0539\n",
      "Step 100 - Average training loss: 0.0501\n",
      "Step 150 - Average training loss: 0.0489\n",
      "Step 200 - Average training loss: 0.0499\n",
      "Average training loss: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:20<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.0653\n",
      "Validation accuracy: 0.9836\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.98      0.99      0.98       794\n",
      "      normal       0.99      0.98      0.98      1076\n",
      "   offensive       0.99      0.98      0.99       698\n",
      "\n",
      "    accuracy                           0.98      2568\n",
      "   macro avg       0.98      0.98      0.98      2568\n",
      "weighted avg       0.98      0.98      0.98      2568\n",
      "\n",
      "New best model saved with accuracy: 0.9836\n",
      "\n",
      "Epoch 4/4\n",
      "Step 50 - Average training loss: 0.0418\n",
      "Step 100 - Average training loss: 0.0440\n",
      "Step 150 - Average training loss: 0.0425\n",
      "Step 200 - Average training loss: 0.0403\n",
      "Average training loss: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:20<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.0603\n",
      "Validation accuracy: 0.9813\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.98      0.98      0.98       794\n",
      "      normal       0.98      0.98      0.98      1076\n",
      "   offensive       0.99      0.98      0.98       698\n",
      "\n",
      "    accuracy                           0.98      2568\n",
      "   macro avg       0.98      0.98      0.98      2568\n",
      "weighted avg       0.98      0.98      0.98      2568\n",
      "\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:10<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 2.2874\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.65      0.02      0.04       628\n",
      "      normal       0.34      0.98      0.51       628\n",
      "   offensive       0.30      0.03      0.05       628\n",
      "\n",
      "    accuracy                           0.34      1884\n",
      "   macro avg       0.43      0.34      0.20      1884\n",
      "weighted avg       0.43      0.34      0.20      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with exp\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f126c695-1097-4ff3-903b-aaf82248fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Python\\py312env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "Step 50 - Average training loss: 1.0529\n",
      "Step 100 - Average training loss: 0.9960\n",
      "Step 150 - Average training loss: 0.9442\n",
      "Step 200 - Average training loss: 0.9174\n",
      "Average training loss: 0.9053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:13<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7748\n",
      "Validation accuracy: 0.6655\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.76      0.71      0.74       794\n",
      "      normal       0.73      0.73      0.73      1076\n",
      "   offensive       0.48      0.52      0.50       698\n",
      "\n",
      "    accuracy                           0.67      2568\n",
      "   macro avg       0.66      0.65      0.65      2568\n",
      "weighted avg       0.67      0.67      0.67      2568\n",
      "\n",
      "New best model saved with accuracy: 0.6655\n",
      "\n",
      "Epoch 2/4\n",
      "Step 50 - Average training loss: 0.7648\n",
      "Step 100 - Average training loss: 0.7619\n",
      "Step 150 - Average training loss: 0.7582\n",
      "Step 200 - Average training loss: 0.7515\n",
      "Average training loss: 0.7497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:13<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7131\n",
      "Validation accuracy: 0.6865\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.80      0.68      0.73       794\n",
      "      normal       0.70      0.84      0.76      1076\n",
      "   offensive       0.54      0.46      0.50       698\n",
      "\n",
      "    accuracy                           0.69      2568\n",
      "   macro avg       0.68      0.66      0.66      2568\n",
      "weighted avg       0.68      0.69      0.68      2568\n",
      "\n",
      "New best model saved with accuracy: 0.6865\n",
      "\n",
      "Epoch 3/4\n",
      "Step 50 - Average training loss: 0.6841\n",
      "Step 100 - Average training loss: 0.6851\n",
      "Step 150 - Average training loss: 0.6838\n",
      "Step 200 - Average training loss: 0.6802\n",
      "Average training loss: 0.6847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:13<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7078\n",
      "Validation accuracy: 0.6990\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.77      0.74      0.76       794\n",
      "      normal       0.69      0.87      0.77      1076\n",
      "   offensive       0.61      0.38      0.47       698\n",
      "\n",
      "    accuracy                           0.70      2568\n",
      "   macro avg       0.69      0.67      0.67      2568\n",
      "weighted avg       0.69      0.70      0.68      2568\n",
      "\n",
      "New best model saved with accuracy: 0.6990\n",
      "\n",
      "Epoch 4/4\n",
      "Step 50 - Average training loss: 0.6182\n",
      "Step 100 - Average training loss: 0.6118\n",
      "Step 150 - Average training loss: 0.6062\n",
      "Step 200 - Average training loss: 0.6105\n",
      "Average training loss: 0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:13<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7147\n",
      "Validation accuracy: 0.7040\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.73      0.83      0.78       794\n",
      "      normal       0.74      0.80      0.77      1076\n",
      "   offensive       0.57      0.41      0.48       698\n",
      "\n",
      "    accuracy                           0.70      2568\n",
      "   macro avg       0.68      0.68      0.68      2568\n",
      "weighted avg       0.69      0.70      0.69      2568\n",
      "\n",
      "New best model saved with accuracy: 0.7040\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:10<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.9052\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.69      0.73       628\n",
      "      normal       0.55      0.79      0.65       628\n",
      "   offensive       0.61      0.41      0.49       628\n",
      "\n",
      "    accuracy                           0.63      1884\n",
      "   macro avg       0.65      0.63      0.62      1884\n",
      "weighted avg       0.65      0.63      0.62      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without exp\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bfaff38-7d24-4564-bc07-21896447ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Python\\py312env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Step 50 - Average training loss: 1.0456\n",
      "Step 100 - Average training loss: 0.9785\n",
      "Step 150 - Average training loss: 0.9376\n",
      "Step 200 - Average training loss: 0.9173\n",
      "Average training loss: 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [00:19<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.8169\n",
      "Validation accuracy: 0.6356\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.61      0.80      0.69      1103\n",
      "      normal       0.67      0.72      0.69      1443\n",
      "   offensive       0.61      0.36      0.45      1107\n",
      "\n",
      "    accuracy                           0.64      3653\n",
      "   macro avg       0.63      0.63      0.61      3653\n",
      "weighted avg       0.63      0.64      0.62      3653\n",
      "\n",
      "New best model saved with accuracy: 0.6356\n",
      "\n",
      "Epoch 2/3\n",
      "Step 50 - Average training loss: 0.7718\n",
      "Step 100 - Average training loss: 0.7802\n",
      "Step 150 - Average training loss: 0.7672\n",
      "Step 200 - Average training loss: 0.7676\n",
      "Average training loss: 0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [00:19<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7628\n",
      "Validation accuracy: 0.6633\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.68      0.72      1103\n",
      "      normal       0.64      0.82      0.72      1443\n",
      "   offensive       0.58      0.44      0.50      1107\n",
      "\n",
      "    accuracy                           0.66      3653\n",
      "   macro avg       0.67      0.65      0.65      3653\n",
      "weighted avg       0.66      0.66      0.66      3653\n",
      "\n",
      "New best model saved with accuracy: 0.6633\n",
      "\n",
      "Epoch 3/3\n",
      "Step 50 - Average training loss: 0.6704\n",
      "Step 100 - Average training loss: 0.6705\n",
      "Step 150 - Average training loss: 0.6665\n",
      "Step 200 - Average training loss: 0.6667\n",
      "Average training loss: 0.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [00:19<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7528\n",
      "Validation accuracy: 0.6729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.77      0.70      0.73      1103\n",
      "      normal       0.69      0.76      0.72      1443\n",
      "   offensive       0.56      0.54      0.55      1107\n",
      "\n",
      "    accuracy                           0.67      3653\n",
      "   macro avg       0.67      0.66      0.67      3653\n",
      "weighted avg       0.67      0.67      0.67      3653\n",
      "\n",
      "New best model saved with accuracy: 0.6729\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:10<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.6538\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.90      0.73      0.81       628\n",
      "      normal       0.65      0.88      0.75       628\n",
      "   offensive       0.72      0.60      0.66       628\n",
      "\n",
      "    accuracy                           0.74      1884\n",
      "   macro avg       0.76      0.74      0.74      1884\n",
      "weighted avg       0.76      0.74      0.74      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55bbe96-9941-4902-a005-9f185a9d63a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/6\n",
      "Step 50 - Average training loss: 0.9396\n",
      "Step 100 - Average training loss: 0.8085\n",
      "Average training loss: 0.7918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5998\n",
      "Validation accuracy: 0.7667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.87      0.82       682\n",
      "      normal       0.81      0.84      0.82       857\n",
      "   offensive       0.44      0.27      0.34       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.68      0.66      0.66      1800\n",
      "weighted avg       0.75      0.77      0.75      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7667\n",
      "\n",
      "Epoch 2/6\n",
      "Step 50 - Average training loss: 0.5659\n",
      "Step 100 - Average training loss: 0.5595\n",
      "Average training loss: 0.5496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5816\n",
      "Validation accuracy: 0.7744\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.81      0.85      0.83       682\n",
      "      normal       0.82      0.84      0.83       857\n",
      "   offensive       0.47      0.36      0.41       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.70      0.68      0.69      1800\n",
      "weighted avg       0.76      0.77      0.77      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7744\n",
      "\n",
      "Epoch 3/6\n",
      "Step 50 - Average training loss: 0.4268\n",
      "Step 100 - Average training loss: 0.4582\n",
      "Average training loss: 0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5627\n",
      "Validation accuracy: 0.7733\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.80      0.88      0.83       682\n",
      "      normal       0.86      0.79      0.82       857\n",
      "   offensive       0.45      0.46      0.45       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.70      0.71      0.70      1800\n",
      "weighted avg       0.78      0.77      0.77      1800\n",
      "\n",
      "\n",
      "Epoch 4/6\n",
      "Step 50 - Average training loss: 0.3655\n",
      "Step 100 - Average training loss: 0.3676\n",
      "Average training loss: 0.3666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6604\n",
      "Validation accuracy: 0.7633\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.89      0.83       682\n",
      "      normal       0.90      0.75      0.81       857\n",
      "   offensive       0.41      0.48      0.44       261\n",
      "\n",
      "    accuracy                           0.76      1800\n",
      "   macro avg       0.70      0.71      0.70      1800\n",
      "weighted avg       0.78      0.76      0.77      1800\n",
      "\n",
      "\n",
      "Epoch 5/6\n",
      "Step 50 - Average training loss: 0.2677\n",
      "Step 100 - Average training loss: 0.2752\n",
      "Average training loss: 0.2821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6423\n",
      "Validation accuracy: 0.7728\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.79      0.86      0.82       682\n",
      "      normal       0.82      0.85      0.83       857\n",
      "   offensive       0.47      0.30      0.37       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.69      0.67      0.67      1800\n",
      "weighted avg       0.76      0.77      0.76      1800\n",
      "\n",
      "\n",
      "Epoch 6/6\n",
      "Step 50 - Average training loss: 0.2045\n",
      "Step 100 - Average training loss: 0.1932\n",
      "Average training loss: 0.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.8712\n",
      "Validation accuracy: 0.7678\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.88      0.83       682\n",
      "      normal       0.85      0.80      0.82       857\n",
      "   offensive       0.44      0.38      0.41       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.69      0.69      0.69      1800\n",
      "weighted avg       0.76      0.77      0.76      1800\n",
      "\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:10<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.8070\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.81      0.66      0.72       628\n",
      "      normal       0.49      0.88      0.63       628\n",
      "   offensive       0.62      0.25      0.35       628\n",
      "\n",
      "    accuracy                           0.59      1884\n",
      "   macro avg       0.64      0.59      0.57      1884\n",
      "weighted avg       0.64      0.59      0.57      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e064b6-096d-4084-9543-11b0f973b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fd7cf3edc44624b8257aa1f4a7bae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d136a096994c4eceaf0e7a1757a94df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a9e3a419ff480ca6686ec897978b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ebb2ff5dde42d7af4954689cc96d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d0b8f1425c43fd81e09269ded589eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Step 50 - Average training loss: 0.8811\n",
      "Step 100 - Average training loss: 0.8549\n",
      "Average training loss: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:10<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6490\n",
      "Validation accuracy: 0.7456\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.83      0.81       682\n",
      "      normal       0.73      0.88      0.80       857\n",
      "   offensive       0.47      0.10      0.16       261\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.66      0.60      0.59      1800\n",
      "weighted avg       0.71      0.75      0.71      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7456\n",
      "\n",
      "Epoch 2/10\n",
      "Step 50 - Average training loss: 0.6471\n",
      "Step 100 - Average training loss: 0.6319\n",
      "Average training loss: 0.6317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6291\n",
      "Validation accuracy: 0.7600\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.73      0.92      0.82       682\n",
      "      normal       0.85      0.76      0.80       857\n",
      "   offensive       0.50      0.34      0.41       261\n",
      "\n",
      "    accuracy                           0.76      1800\n",
      "   macro avg       0.70      0.67      0.68      1800\n",
      "weighted avg       0.76      0.76      0.75      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7600\n",
      "\n",
      "Epoch 3/10\n",
      "Step 50 - Average training loss: 0.5763\n",
      "Step 100 - Average training loss: 0.5824\n",
      "Average training loss: 0.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6324\n",
      "Validation accuracy: 0.7339\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.72      0.85      0.78       682\n",
      "      normal       0.84      0.73      0.78       857\n",
      "   offensive       0.45      0.44      0.45       261\n",
      "\n",
      "    accuracy                           0.73      1800\n",
      "   macro avg       0.67      0.67      0.67      1800\n",
      "weighted avg       0.74      0.73      0.73      1800\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "Step 50 - Average training loss: 0.5361\n",
      "Step 100 - Average training loss: 0.5276\n",
      "Average training loss: 0.5276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6390\n",
      "Validation accuracy: 0.7400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.71      0.88      0.78       682\n",
      "      normal       0.86      0.73      0.79       857\n",
      "   offensive       0.48      0.40      0.43       261\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.68      0.67      0.67      1800\n",
      "weighted avg       0.75      0.74      0.74      1800\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "Step 50 - Average training loss: 0.4400\n",
      "Step 100 - Average training loss: 0.4419\n",
      "Average training loss: 0.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6807\n",
      "Validation accuracy: 0.7450\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.70      0.90      0.79       682\n",
      "      normal       0.84      0.76      0.80       857\n",
      "   offensive       0.51      0.28      0.36       261\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.68      0.65      0.65      1800\n",
      "weighted avg       0.74      0.74      0.73      1800\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "Step 50 - Average training loss: 0.4466\n",
      "Step 100 - Average training loss: 0.4168\n",
      "Average training loss: 0.4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7158\n",
      "Validation accuracy: 0.7800\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.82      0.85      0.83       682\n",
      "      normal       0.81      0.86      0.84       857\n",
      "   offensive       0.47      0.34      0.40       261\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.70      0.68      0.69      1800\n",
      "weighted avg       0.77      0.78      0.77      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7800\n",
      "\n",
      "Epoch 7/10\n",
      "Step 50 - Average training loss: 0.2886\n",
      "Step 100 - Average training loss: 0.2931\n",
      "Average training loss: 0.2934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:10<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7466\n",
      "Validation accuracy: 0.7794\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.87      0.82       682\n",
      "      normal       0.85      0.82      0.84       857\n",
      "   offensive       0.49      0.38      0.43       261\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.71      0.69      0.70      1800\n",
      "weighted avg       0.77      0.78      0.77      1800\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "Step 50 - Average training loss: 0.2344\n",
      "Step 100 - Average training loss: 0.2293\n",
      "Average training loss: 0.2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.8700\n",
      "Validation accuracy: 0.7639\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.72      0.92      0.80       682\n",
      "      normal       0.85      0.79      0.82       857\n",
      "   offensive       0.55      0.27      0.36       261\n",
      "\n",
      "    accuracy                           0.76      1800\n",
      "   macro avg       0.71      0.66      0.66      1800\n",
      "weighted avg       0.76      0.76      0.75      1800\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "Step 50 - Average training loss: 0.1869\n",
      "Step 100 - Average training loss: 0.1919\n",
      "Average training loss: 0.1966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.9329\n",
      "Validation accuracy: 0.7561\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.85      0.81       682\n",
      "      normal       0.85      0.80      0.82       857\n",
      "   offensive       0.40      0.39      0.39       261\n",
      "\n",
      "    accuracy                           0.76      1800\n",
      "   macro avg       0.67      0.68      0.68      1800\n",
      "weighted avg       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      "Step 50 - Average training loss: 0.1869\n",
      "Step 100 - Average training loss: 0.2028\n",
      "Average training loss: 0.2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7476\n",
      "Validation accuracy: 0.7678\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.74      0.91      0.82       682\n",
      "      normal       0.87      0.77      0.82       857\n",
      "   offensive       0.49      0.37      0.42       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.70      0.69      0.69      1800\n",
      "weighted avg       0.77      0.77      0.76      1800\n",
      "\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:11<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.7452\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.75      0.68      0.71       628\n",
      "      normal       0.50      0.84      0.63       628\n",
      "   offensive       0.65      0.26      0.37       628\n",
      "\n",
      "    accuracy                           0.59      1884\n",
      "   macro avg       0.63      0.59      0.57      1884\n",
      "weighted avg       0.63      0.59      0.57      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c9d03a-a34f-47b7-98b5-f2958e87f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/6\n",
      "Step 50 - Average training loss: 0.9250\n",
      "Step 100 - Average training loss: 0.8059\n",
      "Average training loss: 0.7840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5891\n",
      "Validation accuracy: 0.7722\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.90      0.83       682\n",
      "      normal       0.80      0.86      0.83       857\n",
      "   offensive       0.47      0.16      0.24       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.68      0.64      0.63      1800\n",
      "weighted avg       0.74      0.77      0.74      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7722\n",
      "\n",
      "Epoch 2/6\n",
      "Step 50 - Average training loss: 0.5489\n",
      "Step 100 - Average training loss: 0.5241\n",
      "Average training loss: 0.5244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5588\n",
      "Validation accuracy: 0.7761\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.83      0.82      0.82       682\n",
      "      normal       0.77      0.90      0.83       857\n",
      "   offensive       0.51      0.24      0.33       261\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.70      0.66      0.66      1800\n",
      "weighted avg       0.76      0.78      0.76      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7761\n",
      "\n",
      "Epoch 3/6\n",
      "Step 50 - Average training loss: 0.4128\n",
      "Step 100 - Average training loss: 0.4130\n",
      "Average training loss: 0.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5829\n",
      "Validation accuracy: 0.7833\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.80      0.88      0.84       682\n",
      "      normal       0.88      0.80      0.84       857\n",
      "   offensive       0.47      0.49      0.48       261\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.71      0.72      0.72      1800\n",
      "weighted avg       0.79      0.78      0.78      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7833\n",
      "\n",
      "Epoch 4/6\n",
      "Step 50 - Average training loss: 0.2807\n",
      "Step 100 - Average training loss: 0.2908\n",
      "Average training loss: 0.2887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7101\n",
      "Validation accuracy: 0.7600\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.80      0.85      0.82       682\n",
      "      normal       0.88      0.78      0.83       857\n",
      "   offensive       0.39      0.48      0.43       261\n",
      "\n",
      "    accuracy                           0.76      1800\n",
      "   macro avg       0.69      0.70      0.69      1800\n",
      "weighted avg       0.78      0.76      0.77      1800\n",
      "\n",
      "\n",
      "Epoch 5/6\n",
      "Step 50 - Average training loss: 0.1861\n",
      "Step 100 - Average training loss: 0.1922\n",
      "Average training loss: 0.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.8168\n",
      "Validation accuracy: 0.7506\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.77      0.87      0.82       682\n",
      "      normal       0.87      0.76      0.81       857\n",
      "   offensive       0.38      0.42      0.40       261\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.67      0.68      0.68      1800\n",
      "weighted avg       0.76      0.75      0.75      1800\n",
      "\n",
      "\n",
      "Epoch 6/6\n",
      "Step 50 - Average training loss: 0.1158\n",
      "Step 100 - Average training loss: 0.1339\n",
      "Average training loss: 0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.9502\n",
      "Validation accuracy: 0.7528\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.81      0.82      0.81       682\n",
      "      normal       0.86      0.78      0.82       857\n",
      "   offensive       0.38      0.48      0.42       261\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.68      0.69      0.69      1800\n",
      "weighted avg       0.77      0.75      0.76      1800\n",
      "\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:10<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.9773\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.84      0.59      0.70       628\n",
      "      normal       0.50      0.84      0.63       628\n",
      "   offensive       0.60      0.37      0.45       628\n",
      "\n",
      "    accuracy                           0.60      1884\n",
      "   macro avg       0.65      0.60      0.59      1884\n",
      "weighted avg       0.65      0.60      0.59      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with explainations\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b2ddb8-cc5b-41bc-96d5-837292b4c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting training dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\MachineLearning\\AICU\\recommendation_system\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Step 50 - Average training loss: 0.8845\n",
      "Step 100 - Average training loss: 0.7942\n",
      "Average training loss: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.6443\n",
      "Validation accuracy: 0.7500\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.79      0.80      0.80       682\n",
      "      normal       0.74      0.90      0.81       857\n",
      "   offensive       0.52      0.13      0.20       261\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.68      0.61      0.60      1800\n",
      "weighted avg       0.73      0.75      0.72      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7500\n",
      "\n",
      "Epoch 2/3\n",
      "Step 50 - Average training loss: 0.5991\n",
      "Step 100 - Average training loss: 0.5844\n",
      "Average training loss: 0.5751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5825\n",
      "Validation accuracy: 0.7683\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.80      0.84      0.82       682\n",
      "      normal       0.81      0.86      0.83       857\n",
      "   offensive       0.43      0.29      0.35       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.68      0.66      0.67      1800\n",
      "weighted avg       0.75      0.77      0.76      1800\n",
      "\n",
      "New best model saved with accuracy: 0.7683\n",
      "\n",
      "Epoch 3/3\n",
      "Step 50 - Average training loss: 0.4721\n",
      "Step 100 - Average training loss: 0.4923\n",
      "Average training loss: 0.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:09<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5793\n",
      "Validation accuracy: 0.7683\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.79      0.86      0.82       682\n",
      "      normal       0.82      0.83      0.83       857\n",
      "   offensive       0.45      0.32      0.37       261\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.68      0.67      0.67      1800\n",
      "weighted avg       0.75      0.77      0.76      1800\n",
      "\n",
      "\n",
      "Evaluating model on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:10<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.2974\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.79      0.63      0.70       628\n",
      "      normal       0.47      0.89      0.62       628\n",
      "   offensive       0.67      0.21      0.32       628\n",
      "\n",
      "    accuracy                           0.58      1884\n",
      "   macro avg       0.65      0.58      0.55      1884\n",
      "weighted avg       0.65      0.58      0.55      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with explainations 3 epochs\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ae9a0d-1887-465e-80c6-094e38fc6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a function to generate the classification report using the saved model\n",
    "def generate_test_report(test_df, tokenizer, model, le, device):\n",
    "    # Preprocess test data\n",
    "    test_texts = test_df['tweet_text']\n",
    "    test_labels = test_df['label']\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=24)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(all_true_labels, all_predictions, target_names=le.classes_))\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load the saved model\n",
    "        model = BertForSequenceClassification.from_pretrained('fine_tuned_bert')\n",
    "        tokenizer = BertTokenizer.from_pretrained('fine_tuned_bert')\n",
    "\n",
    "        # Load the label encoder\n",
    "        le = np.load('label_classes.npy', allow_pickle=True)\n",
    "\n",
    "        # Load TEST_DF (ensure it's already preprocessed as mentioned earlier)\n",
    "        test_df = TEST_DF\n",
    "        test_df = test_df.dropna(subset=['tweet_text', 'label'])\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "        test_df['label'] = le.transform(test_df['label'])\n",
    "\n",
    "        # Generate the test report\n",
    "        generate_test_report(test_df, tokenizer, model, le, device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        print(\"\\nFull error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec81c3f9-1afd-4fc2-b2e8-cb17c96341cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error occurred: 'numpy.ndarray' object has no attribute 'transform'\n",
      "\n",
      "Full error details:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_34924\\4269899375.py\", line 58, in main\n",
      "    test_df['label'] = le.transform(test_df['label'])\n",
      "                       ^^^^^^^^^^^^\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'transform'. Did you mean: 'transpose'?\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b0e8f-577f-4709-a6c8-9793a7c6ce4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
