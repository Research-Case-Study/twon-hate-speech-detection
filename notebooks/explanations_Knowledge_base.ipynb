{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e028bccb-12db-426e-8ef5-d4e1f8e60b9e",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to experiment with how the explainations are created.\n",
    "In section 1, we first initialize functions for (loading llm, connecting vector db, and en embeddings model)\n",
    "In section 2, we experiment and run this flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567b13d-37da-4ccb-8893-6d5527897833",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3f356-7ed9-4538-8e18-cc28a3aea83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install sentence_transformers \n",
    "!pip install qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c51813-435f-46fa-ba29-c8f30da86341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, Batch\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Change to the parent directory of the notebook\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de704bea-b691-4f38-8d4d-db1b9f0fe2a9",
   "metadata": {},
   "source": [
    "Available Models:\n",
    "\n",
    "\"llama3.1:8b-instruct-q6_K\", \n",
    "\"gemma:7b-instruct-q6_K\",\n",
    "\"qwen2:72b-instruct-q6_K\",\n",
    "\"llama3.1:70b-instruct-q6_K\",\n",
    "\"phi3:14b-medium-128k-instruct-q6_K\",\n",
    "\"mixtral:8x7b-instruct-v0.1-q6_K\",\n",
    "\"mistral:7b-instruct-v0.2-q6_K\",\n",
    "\"llama3.3:70b-instruct-q6_K\",\n",
    "\"phi3.5:3.8b-mini-instruct-q6_K\",\n",
    "\"gemma2:27b-instruct-q6_K\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3110e4-b57e-4379-8dde-3ac93b2e176b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81de4bb-5d53-4c15-ae64-4c8269a426e0",
   "metadata": {},
   "source": [
    "## Load Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1d121c-87cb-4693-8f85-fdb530925264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDINGS_MODEL = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a38c7d-b7cc-4e08-a974-3b8bd11f180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY USE THIS IF YOU HAVE CUDA (NVIDIA GPU with 9 GB VRAM available:\n",
    "\n",
    "EMBEDDINGS_MODEL = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7c9c00-2c4b-4168-ace9-65cf3e8e6387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 281 ms\n",
      "Wall time: 262 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.3991723,  2.6841042,  2.4770339, ..., -1.4864485,  1.8343586,\n",
       "        1.2716196], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDINGS_MODEL.encode(\"# testing how much time embeddings model take on your system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb486448-c637-4fd3-8361-8a7a56557fcd",
   "metadata": {},
   "source": [
    "## Configure Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d88b83-d2c8-4e1d-949d-6532420dacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='HateXplain_index_2'), CollectionDescription(name='test_index_'), CollectionDescription(name='HateXplain_index_3'), CollectionDescription(name='test_index'), CollectionDescription(name='HateXplain_index_1')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://7ef18c4d-2ef6-4fb0-9243-0ac62546593c.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"BR8zsNr5lEYrqJPL4EknUj2oRska2JO1nHwPFawlFMqZIrYMuGZ0Wg\",\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a407b5-d82a-475a-8bd2-438d5c01b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=index_name,\n",
    "        vectors_config=VectorParams(size=len(EMBEDDINGS_MODEL.encode(\"\")), distance=Distance.DOT)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c06ab9-fd82-401c-a396-40020d9ab9f1",
   "metadata": {},
   "source": [
    "## Configure LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35261a1b-2919-4b19-ac32-d2979a59ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://inf.cl.uni-trier.de/chat/\"\n",
    "\n",
    "def llm(model_name, system_prompt, input_query):\n",
    "    # Construct the request payload\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"content\": system_prompt, \"role\": \"system\"},\n",
    "            {\"content\": input_query, \"role\": \"user\"}\n",
    "        ],\n",
    "        \"model\": model_name,\n",
    "        \"options\": {}\n",
    "    }\n",
    "    \n",
    "    # Set the request headers\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Send the POST request\n",
    "    response = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    # Process the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Response received successfully:\")\n",
    "        response = response.json() # json.dumps(, indent=4)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve response. Status code: {response.status_code}\")\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ec57d-ae1c-45e8-9c51-79943ee25924",
   "metadata": {},
   "source": [
    "## Load Dataset and convert to DF | Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ee6ef3-f276-4773-ac21-13a3a398d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.json file\n",
    "file_path = 'Data/dataset.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4380f977-fe8f-40c3-9aef-f997c9e1a6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>key_features</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "      <th>annotator_1_label</th>\n",
       "      <th>annotator_1_target</th>\n",
       "      <th>annotator_2_label</th>\n",
       "      <th>annotator_2_target</th>\n",
       "      <th>annotator_3_label</th>\n",
       "      <th>annotator_3_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>9989999_gab</td>\n",
       "      <td>if ur still on twitter tell carlton i said his...</td>\n",
       "      <td>[alcoholic, drug, addicted, teenage, single, m...</td>\n",
       "      <td>Men, None, Other, Women</td>\n",
       "      <td>offensive</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Men, Women, Other</td>\n",
       "      <td>offensive</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>9990225_gab</td>\n",
       "      <td>when i first got on here and said i hate trump...</td>\n",
       "      <td>[i, hate, trump, i, was, hating, niggers]</td>\n",
       "      <td>African, None, Other</td>\n",
       "      <td>offensive</td>\n",
       "      <td>offensive</td>\n",
       "      <td>African</td>\n",
       "      <td>offensive</td>\n",
       "      <td>African, Other</td>\n",
       "      <td>normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20145</th>\n",
       "      <td>9991681_gab</td>\n",
       "      <td>was macht der moslem wenn der zion gegen seine...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Islam, None, Other</td>\n",
       "      <td>normal</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Islam</td>\n",
       "      <td>normal</td>\n",
       "      <td>Other</td>\n",
       "      <td>normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20146</th>\n",
       "      <td>9992513_gab</td>\n",
       "      <td>it is awful look at world demographics asians ...</td>\n",
       "      <td>[asians, are, fucking, everywhere, white, geno...</td>\n",
       "      <td>Asian, Hispanic</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Asian</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>9998729_gab</td>\n",
       "      <td>the jewish globalist elite have only imported ...</td>\n",
       "      <td>[imported, few, million, muslims, to, multicul...</td>\n",
       "      <td>African, Islam, Jewish</td>\n",
       "      <td>offensive</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>African, Islam</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Islam, Jewish</td>\n",
       "      <td>offensive</td>\n",
       "      <td>African, Islam, Jewish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           post_id                                         tweet_text  \\\n",
       "20143  9989999_gab  if ur still on twitter tell carlton i said his...   \n",
       "20144  9990225_gab  when i first got on here and said i hate trump...   \n",
       "20145  9991681_gab  was macht der moslem wenn der zion gegen seine...   \n",
       "20146  9992513_gab  it is awful look at world demographics asians ...   \n",
       "20147  9998729_gab  the jewish globalist elite have only imported ...   \n",
       "\n",
       "                                            key_features  \\\n",
       "20143  [alcoholic, drug, addicted, teenage, single, m...   \n",
       "20144          [i, hate, trump, i, was, hating, niggers]   \n",
       "20145                                                 []   \n",
       "20146  [asians, are, fucking, everywhere, white, geno...   \n",
       "20147  [imported, few, million, muslims, to, multicul...   \n",
       "\n",
       "                        target       label annotator_1_label  \\\n",
       "20143  Men, None, Other, Women   offensive         offensive   \n",
       "20144     African, None, Other   offensive         offensive   \n",
       "20145       Islam, None, Other      normal         offensive   \n",
       "20146          Asian, Hispanic  hatespeech        hatespeech   \n",
       "20147   African, Islam, Jewish   offensive        hatespeech   \n",
       "\n",
       "      annotator_1_target annotator_2_label annotator_2_target  \\\n",
       "20143  Men, Women, Other         offensive               None   \n",
       "20144            African         offensive     African, Other   \n",
       "20145              Islam            normal              Other   \n",
       "20146           Hispanic        hatespeech              Asian   \n",
       "20147     African, Islam         offensive      Islam, Jewish   \n",
       "\n",
       "      annotator_3_label      annotator_3_target  \n",
       "20143            normal                    None  \n",
       "20144            normal                    None  \n",
       "20145            normal                    None  \n",
       "20146         offensive                   Asian  \n",
       "20147         offensive  African, Islam, Jewish  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "rows = []\n",
    "for row in data.items():\n",
    "    post_id = row[1]['post_id']\n",
    "    post_tokens = row[1]['post_tokens']\n",
    "    rationales = row[1]['rationales']\n",
    "    annotators = row[1]['annotators']\n",
    "    tweet_text = \" \".join(post_tokens)\n",
    "\n",
    "    # Aggregate all rationales\n",
    "    combined_rationale = [any(rat[i] for rat in rationales) for i in range(len(post_tokens))] if rationales else [0] * len(post_tokens)\n",
    "\n",
    "    # Extract key features\n",
    "    key_features = [post_tokens[i] for i, is_key in enumerate(combined_rationale) if is_key]\n",
    "\n",
    "    annotator_data = {}\n",
    "    all_targets = set()  # To store unique targets\n",
    "    labels = []  # To collect labels for voting\n",
    "\n",
    "    for index, annotator in enumerate(annotators, start=1):  # Use index as annotator_id\n",
    "        annotator_data[f\"annotator_{index}_label\"] = annotator['label']\n",
    "        annotator_data[f\"annotator_{index}_target\"] = \", \".join(annotator['target']) if annotator['target'] else None\n",
    "\n",
    "        # Collect all targets\n",
    "        all_targets.update(annotator['target'])\n",
    "\n",
    "        # Collect labels for voting\n",
    "        labels.append(annotator['label'])\n",
    "\n",
    "    # Combine all unique targets\n",
    "    combined_target = \", \".join(sorted(all_targets))\n",
    "\n",
    "    # Perform majority voting for the label\n",
    "    label_counts = Counter(labels)\n",
    "    voted_label = label_counts.most_common(1)[0][0] if labels else None\n",
    "\n",
    "    # Create the row\n",
    "    row_data = {\n",
    "        \"post_id\": post_id,\n",
    "        \"tweet_text\": tweet_text,\n",
    "        \"key_features\": key_features,\n",
    "        \"target\": combined_target,\n",
    "        \"label\": voted_label,\n",
    "    }\n",
    "    row_data.update(annotator_data)  # Add annotator-related fields\n",
    "\n",
    "    rows.append(row_data)\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03a4fe4-5a4b-401e-8601-4c14393cd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/dataset.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46c6f77c-26b2-424b-a44e-651a8392f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filder data\n",
    "data = [\n",
    "    {key: d[key] for key in ('post_id', 'tweet_text', 'key_features', 'target', 'label')}\n",
    "    for d in rows\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1902ecc-f262-45e9-adb8-281a9793b539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'post_id': '13851720_gab',\n",
       " 'tweet_text': 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate',\n",
       " 'key_features': ['disgusting',\n",
       "  'kike',\n",
       "  'language',\n",
       "  'exterminate',\n",
       "  'the',\n",
       "  'goyim'],\n",
       " 'target': 'Jewish',\n",
       " 'label': 'hatespeech'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see data\n",
    "data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b0685-c62f-4ac7-9f02-ea33fb85d236",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18839-ffed-4b3b-beb3-9ccbe60a6cd1",
   "metadata": {},
   "source": [
    "Here we make experiments with system prompt, \n",
    "\n",
    "System prompt contains : what model is supposed to do with the input query. with an exmple output.\n",
    "Input Query contains, one row of dataset.\n",
    "explaination variable will have the output of model which will be an explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac2453b-a98f-4631-8cf8-cb46e49d5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing LLM\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "input_query = \"Hi\"\n",
    "model_name = \"llama3.3:70b-instruct-q6_K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14604fb-0006-4238-b022-e20f502311de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received successfully:\n",
      "How can I assist you today? Do you have any questions, need help with something, or just want to chat? I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "response = llm(model_name, system_prompt, input_query)\n",
    "explaination = response['response']\n",
    "print(explaination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78aadd8d-4c88-4560-be3e-2c84ca94d317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['normal', 'hatespeech', 'offensive'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "997d9393-967b-4eaa-88e9-bfa667dea15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"post_id\": \"13851720_gab\", \"tweet_text\": \"laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate\", \"key_features\": [\"disgusting\", \"kike\", \"language\", \"exterminate\", \"the\", \"goyim\"], \"target\": \"Jewish\", \"label\": \"hatespeech\"}'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking one row from dataset and converting it into string.\n",
    "user_input = json.dumps(data[5])\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73a34dbd-acc9-4a4a-9726-6955e5ac5c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received successfully:\n",
      "The given tweet text is labeled as hate speech, and upon examination, it's clear why. Here are the reasons why this text can be considered hate speech:\n",
      "\n",
      "1. **Derogatory Language**: The use of the term \"kike\" is a strong derogatory slur aimed at Jewish people. This word has a long history of being used in an anti-Semitic context to demean and dehumanize Jews.\n",
      "\n",
      "2. **Violent Imagery**: The text mentions \"raped me,\" which introduces a violent act, but more critically, it combines this with the notion of being screamed at \"in her disgusting kike language.\" This not only invokes violence but also uses the victim's experience as a backdrop to express hatred towards the Jewish community by focusing on their language and ethnicity in a demeaning manner.\n",
      "\n",
      "3. **Call for Extermination**: The phrase \"we must exterminate the goyim\" is particularly alarming. \"Goyim\" refers to non-Jews, and calling for their extermination is a direct expression of genocidal intent. This kind of rhetoric is reminiscent of historical atrocities committed against various groups, including the Holocaust, where similar calls for extermination were made.\n",
      "\n",
      "4. **Dehumanizing Language**: The description of someone's language as \"disgusting\" because it is associated with their ethnicity (\"kike language\") dehumanizes the individual and, by extension, the entire group they belong to. This kind of language contributes to a climate where discrimination and violence against the targeted group are more likely.\n",
      "\n",
      "5. **Specific Targeting**: The tweet specifically targets Jewish people through the use of derogatory terms and calls for violence against non-Jews, indicating a clear intent to spread hate towards the Jewish community.\n",
      "\n",
      "In conclusion, this text is considered hate speech because it employs derogatory language, invokes violent imagery, includes a call for extermination based on religious/ethnic identity, uses dehumanizing language, and specifically targets a group (in this case, Jewish people) with the intent to demean, threaten, or incite violence against them. The presence of these elements justifies the label of hate speech.\n"
     ]
    }
   ],
   "source": [
    "# Testing LLM\n",
    "system_prompt = \"\"\"\n",
    "You are supposed to write an explaination to user's input explaining why ITS hate speech and why its not hate speech, based on the information provided. Our annotators has already decided if a text is a hate speech or not, and labeled.\n",
    "you will be given\n",
    "tweet_text : \n",
    "\n",
    "key_features: \n",
    "\n",
    "{'post_id': ## post id . ignore it\n",
    " 'tweet_text': #its the main user text that you need to write explaination on.\n",
    " 'key_features': [] # list of the words that made a decision, important feature\n",
    " 'target': # targetted audience. \n",
    " 'label': # either offensive, hate speech or normal.   \n",
    " }\n",
    "\n",
    " If label is neutral and you think the text is hate speech. you still have to explain why this text is not hate speech. \n",
    " if the label is offensive, you judge how its offensive based on text and keyfeatures etc.\n",
    "\n",
    "Now you will be given user input and you have to Write explainations.\n",
    "\"\"\"\n",
    "\n",
    "input_query = user_input\n",
    "model_name = \"llama3.3:70b-instruct-q6_K\"\n",
    "\n",
    "response = llm(model_name, system_prompt, input_query)\n",
    "explaination = response['response']\n",
    "print(explaination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb05ff7-6265-487a-aa15-12d87f0e171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in data.items():\n",
    "#     print(a.key())\n",
    "#     break\n",
    "# # a[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac3b153-2243-49f7-893d-5c9360374bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[1]['annotators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c0d230-e860-4dfb-bf05-13d3266a011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d5fe5-ed18-4051-bd16-734267230613",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f7f80-0a43-405c-9fd5-ef8ac89ef043",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c3c59fd-2546-4027-a3d9-0c123b8ddc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"HateXplain_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47002354-dd3a-41b2-992e-83f75d4cca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93359e06-1cea-417a-a777-8c08450bb0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='HateXplain_index_2'), CollectionDescription(name='test_index_'), CollectionDescription(name='HateXplain_index_3'), CollectionDescription(name='HateXplain_index'), CollectionDescription(name='HateXplain_index_4'), CollectionDescription(name='test_index'), CollectionDescription(name='HateXplain_index_1')]\n"
     ]
    }
   ],
   "source": [
    "print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebee5a2e-7468-445f-b999-80cbd27511a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload data into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dcaee8c-22d0-4a18-a220-2879f722ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e83a1923-aa47-4baa-ac1e-5b3d72137acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_in_qdrant_collection(data_list, data_list_embeddings, ids):\n",
    "    try:\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=index_name,\n",
    "            points=Batch(\n",
    "                ids=ids,\n",
    "                vectors=data_list_embeddings,\n",
    "                payloads=data_list\n",
    "            ),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # traceback.print_exc()\n",
    "        print(f\"Exception in create_embeddings_and_upsert {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d7e4f32-c0a6-4092-9d5b-792c1575d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_and_upsert(batch_size=1000):\n",
    "\n",
    "    print(\"Creating rows embeddings\")\n",
    "    ids = list(range(1, len(data) + 1))\n",
    "    \n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch_data = data[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        batch_data_list_embeddings = []\n",
    "        for row in batch_data:\n",
    "            payload = (\n",
    "                f\"tweet_text: {row['tweet_text']}\\n\"\n",
    "                f\"key_features: {row['key_features']}\\n\"\n",
    "                f\"target: {row['target']}\\n\"\n",
    "                f\"label: {row['label']}\\n\"\n",
    "                # WE add EXPLAINATIONS HERE\n",
    "                # f\"post_id: {row['post_id']}\"\n",
    "            )\n",
    "            \n",
    "            batch_data_list_embeddings.append(EMBEDDINGS_MODEL.encode(payload))\n",
    "            #batch_ids.append(row['post_id'])\n",
    "        \n",
    "\n",
    "        # Call the Qdrant function\n",
    "        upsert_in_qdrant_collection(batch_data, batch_data_list_embeddings, batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58927144-418f-49a0-968f-415dda313982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rows embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [27:57<00:00,  8.30s/it]\n"
     ]
    }
   ],
   "source": [
    "create_embeddings_and_upsert(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344a801-805a-4e8a-8715-b1392428a5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7121406-68f5-45a2-84f1-1c9bc964bdf2",
   "metadata": {},
   "source": [
    "## Query RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c87b359-e21c-4d6e-b655-38b0d79186d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"loomergate\"\n",
    "top_k = 3\n",
    "embedding = EMBEDDINGS_MODEL.encode(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afe8c186-f7c2-4b3c-86c6-1347ea59a612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=17182, version=171, score=918.36975, payload={'post_id': '22049484_gab', 'tweet_text': 'is there any opposition to muslim invasion in sweden', 'key_features': [], 'target': 'Islam', 'label': 'normal'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=16153, version=161, score=892.71375, payload={'post_id': '18469603_gab', 'tweet_text': 'how boomers let jews loot the usa', 'key_features': [], 'target': 'Jewish, Men, None, Women', 'label': 'normal'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=17452, version=174, score=876.84314, payload={'post_id': '22918483_gab', 'tweet_text': 'reminder that google is run by jews', 'key_features': [], 'target': 'Jewish, None', 'label': 'normal'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=11898, version=118, score=873.79474, payload={'post_id': '1167982348769660928_twitter', 'tweet_text': 'everyone in your life is a fucking lie i promise', 'key_features': [], 'target': 'Men, None, Women', 'label': 'normal'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=13177, version=131, score=867.65, payload={'post_id': '1227305288501465089_twitter', 'tweet_text': '<user> syrian muslims suffer a lots', 'key_features': [], 'target': 'Arab, Islam, None', 'label': 'normal'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = qdrant_client.search(\n",
    "    collection_name=index_name, query_vector=embedding, limit=5\n",
    ")\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34614d-7351-44e7-95c7-4abd63ee785b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
