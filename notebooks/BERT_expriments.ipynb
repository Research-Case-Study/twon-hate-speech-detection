{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_steps = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc='Training'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        val_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                val_accuracy += (predictions == labels).sum().item()\n",
    "                val_steps += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_steps\n",
    "        accuracy = val_accuracy / (val_steps * train_loader.batch_size)\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    'C:/Users/Nilofar/Desktop/twon-hate-speech-detection/Data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>key_features</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "      <th>annotator_1_label</th>\n",
       "      <th>annotator_1_target</th>\n",
       "      <th>annotator_2_label</th>\n",
       "      <th>annotator_2_target</th>\n",
       "      <th>annotator_3_label</th>\n",
       "      <th>annotator_3_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>['ching', 'chong']</td>\n",
       "      <td>Asian</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Asian</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Asian</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>['bitch', 'i', 'hate', 'white', 'bitches']</td>\n",
       "      <td>Caucasian, Women</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Caucasian, Women</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Women, Caucasian</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Women, Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>13851720_gab</td>\n",
       "      <td>laura loomer raped me while screaming at me in...</td>\n",
       "      <td>['disgusting', 'kike', 'language', 'exterminat...</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>24198545_gab</td>\n",
       "      <td>and this is why i end up with nigger trainee d...</td>\n",
       "      <td>['nigger', 'trainee', 'who', 'can', 'not', 'sp...</td>\n",
       "      <td>African</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>African</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>African</td>\n",
       "      <td>offensive</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>14567516_gab</td>\n",
       "      <td>nogs jews and dykes how enriching</td>\n",
       "      <td>['nogs', 'jews', 'and', 'dykes']</td>\n",
       "      <td>African, Homosexual, Jewish</td>\n",
       "      <td>offensive</td>\n",
       "      <td>offensive</td>\n",
       "      <td>African, Jewish, Homosexual</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>African, Jewish, Homosexual</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      post_id  \\\n",
       "3           3  1179088797964763136_twitter   \n",
       "4           4  1179085312976445440_twitter   \n",
       "5           5                 13851720_gab   \n",
       "6           6                 24198545_gab   \n",
       "7           7                 14567516_gab   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "3  <user> i am bit confused coz chinese ppl can n...   \n",
       "4  this bitch in whataburger eating a burger with...   \n",
       "5  laura loomer raped me while screaming at me in...   \n",
       "6  and this is why i end up with nigger trainee d...   \n",
       "7                  nogs jews and dykes how enriching   \n",
       "\n",
       "                                        key_features  \\\n",
       "3                                 ['ching', 'chong']   \n",
       "4         ['bitch', 'i', 'hate', 'white', 'bitches']   \n",
       "5  ['disgusting', 'kike', 'language', 'exterminat...   \n",
       "6  ['nigger', 'trainee', 'who', 'can', 'not', 'sp...   \n",
       "7                   ['nogs', 'jews', 'and', 'dykes']   \n",
       "\n",
       "                        target       label annotator_1_label  \\\n",
       "3                        Asian  hatespeech        hatespeech   \n",
       "4             Caucasian, Women  hatespeech        hatespeech   \n",
       "5                       Jewish  hatespeech        hatespeech   \n",
       "6                      African  hatespeech        hatespeech   \n",
       "7  African, Homosexual, Jewish   offensive         offensive   \n",
       "\n",
       "            annotator_1_target annotator_2_label           annotator_2_target  \\\n",
       "3                        Asian         offensive                        Asian   \n",
       "4             Caucasian, Women        hatespeech             Women, Caucasian   \n",
       "5                       Jewish        hatespeech                       Jewish   \n",
       "6                      African        hatespeech                      African   \n",
       "7  African, Jewish, Homosexual        hatespeech  African, Jewish, Homosexual   \n",
       "\n",
       "  annotator_3_label annotator_3_target  \n",
       "3        hatespeech              Asian  \n",
       "4         offensive   Women, Caucasian  \n",
       "5        hatespeech             Jewish  \n",
       "6         offensive            African  \n",
       "7         offensive             Jewish  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Cleaning dataset...\n",
      "Dataset shape after cleaning: (20148, 12)\n",
      "\n",
      "Class Distribution:\n",
      "hatespeech: 6234 samples\n",
      "normal: 8153 samples\n",
      "offensive: 5761 samples\n",
      "\n",
      "Splitting dataset...\n",
      "Initializing BERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Creating dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1008/1008 [1:17:15<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 252/252 [00:45<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7526\n",
      "Validation accuracy: 0.6739\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.73      0.76      0.74      1247\n",
      "      normal       0.66      0.81      0.73      1631\n",
      "   offensive       0.60      0.39      0.48      1152\n",
      "\n",
      "    accuracy                           0.67      4030\n",
      "   macro avg       0.67      0.65      0.65      4030\n",
      "weighted avg       0.67      0.67      0.66      4030\n",
      "\n",
      "New best model saved with accuracy: 0.6739\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1008/1008 [2:02:36<00:00,  7.30s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.6548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 252/252 [01:49<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.7476\n",
      "Validation accuracy: 0.6792\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.78      0.71      0.74      1247\n",
      "      normal       0.73      0.73      0.73      1631\n",
      "   offensive       0.52      0.57      0.54      1152\n",
      "\n",
      "    accuracy                           0.68      4030\n",
      "   macro avg       0.68      0.67      0.67      4030\n",
      "weighted avg       0.69      0.68      0.68      4030\n",
      "\n",
      "New best model saved with accuracy: 0.6792\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1008/1008 [1:25:58<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 252/252 [01:19<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.8852\n",
      "Validation accuracy: 0.6516\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech       0.73      0.74      0.74      1247\n",
      "      normal       0.76      0.62      0.68      1631\n",
      "   offensive       0.48      0.60      0.53      1152\n",
      "\n",
      "    accuracy                           0.65      4030\n",
      "   macro avg       0.66      0.65      0.65      4030\n",
      "weighted avg       0.67      0.65      0.66      4030\n",
      "\n",
      "\n",
      "Saving model and tokenizer...\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        # Convert to lists to ensure indexing works correctly\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "        self.labels = labels.tolist() if hasattr(labels, 'tolist') else list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Verify data integrity\n",
    "        assert len(self.texts) == len(\n",
    "            self.labels), \"Texts and labels must have the same length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.texts):\n",
    "            raise IndexError(\n",
    "                f\"Index {idx} out of bounds for dataset of size {len(self.texts)}\")\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, le, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_steps = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc='Training'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                val_steps += 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_val_loss = val_loss / val_steps\n",
    "        accuracy = (np.array(all_predictions) ==\n",
    "                    np.array(all_true_labels)).mean()\n",
    "\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation accuracy: {accuracy:.4f}')\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(all_true_labels, all_predictions,\n",
    "                                    target_names=le.classes_))\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'New best model saved with accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Check if CUDA is available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "        # Load and preprocess data\n",
    "        print(\"Loading dataset...\")\n",
    "        df = pd.read_csv(\n",
    "            'C:/Users/Nilofar/Desktop/twon-hate-speech-detection/Data/dataset.csv')\n",
    "\n",
    "        # Data validation\n",
    "        required_columns = ['tweet_text', 'label']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(\n",
    "                f\"Dataset must contain columns: {required_columns}\")\n",
    "\n",
    "        # Remove any NaN values\n",
    "        print(\"Cleaning dataset...\")\n",
    "        df = df.dropna(subset=['tweet_text', 'label'])\n",
    "\n",
    "        # Reset index after dropping NaN values\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        print(f\"Dataset shape after cleaning: {df.shape}\")\n",
    "\n",
    "        # Convert labels to numerical values\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "        # Print class distribution\n",
    "        print(\"\\nClass Distribution:\")\n",
    "        for i, label in enumerate(le.classes_):\n",
    "            count = len(df[df['label'] == i])\n",
    "            print(f\"{label}: {count} samples\")\n",
    "\n",
    "        # Split the dataset\n",
    "        print(\"\\nSplitting dataset...\")\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['tweet_text'],\n",
    "            df['label'],\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=df['label']\n",
    "        )\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        print(\"Initializing BERT model and tokenizer...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        num_labels = 3\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        print(\"Creating datasets...\")\n",
    "        train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        # Create dataloaders\n",
    "        print(\"Creating dataloaders...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=16\n",
    "        )\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        model.to(device)\n",
    "\n",
    "        # Train the model\n",
    "        print(\"\\nStarting training...\")\n",
    "        train_model(model, train_loader, val_loader, device, le)\n",
    "\n",
    "        # Save the final model and tokenizer\n",
    "        print(\"\\nSaving model and tokenizer...\")\n",
    "        model.save_pretrained('fine_tuned_bert')\n",
    "        tokenizer.save_pretrained('fine_tuned_bert')\n",
    "        np.save('label_classes.npy', le.classes_)\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        print(\"\\nFull error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU (Python 3.11)",
   "language": "python",
   "name": "pytorch-gpu-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
