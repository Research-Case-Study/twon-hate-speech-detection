{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9567b13d-37da-4ccb-8893-6d5527897833",
      "metadata": {
        "id": "9567b13d-37da-4ccb-8893-6d5527897833"
      },
      "source": [
        "# Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02a3f356-7ed9-4538-8e18-cc28a3aea83b",
      "metadata": {
        "id": "02a3f356-7ed9-4538-8e18-cc28a3aea83b"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install sentence_transformers\n",
        "!pip install qdrant_client\n",
        "!pip install einops\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45c51813-435f-46fa-ba29-c8f30da86341",
      "metadata": {
        "id": "45c51813-435f-46fa-ba29-c8f30da86341"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams, Batch\n",
        "from collections import Counter\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42201bf0-bd14-4ffe-b6d8-39cb6ba05ce1",
      "metadata": {
        "id": "42201bf0-bd14-4ffe-b6d8-39cb6ba05ce1"
      },
      "source": [
        "# LLM for Explaination Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c06ab9-fd82-401c-a396-40020d9ab9f1",
      "metadata": {
        "id": "07c06ab9-fd82-401c-a396-40020d9ab9f1"
      },
      "source": [
        "## Configure LLM"
      ]
    },
    {
      "cell_type": "raw",
      "id": "de704bea-b691-4f38-8d4d-db1b9f0fe2a9",
      "metadata": {
        "id": "de704bea-b691-4f38-8d4d-db1b9f0fe2a9"
      },
      "source": [
        "Bellow Experiments, you can select any of these models by copy pasting the name in model_name\n",
        "\n",
        "Available Models:\n",
        "\n",
        "\"llama3.1:8b-instruct-q6_K\",\n",
        "\"gemma:7b-instruct-q6_K\",\n",
        "\"qwen2:72b-instruct-q6_K\",\n",
        "\"llama3.1:70b-instruct-q6_K\",\n",
        "\"phi3:14b-medium-128k-instruct-q6_K\",\n",
        "\"mixtral:8x7b-instruct-v0.1-q6_K\",\n",
        "\"mistral:7b-instruct-v0.2-q6_K\",\n",
        "\"llama3.3:70b-instruct-q6_K\",\n",
        "\"phi3.5:3.8b-mini-instruct-q6_K\",\n",
        "\"gemma2:27b-instruct-q6_K\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "35261a1b-2919-4b19-ac32-d2979a59ff07",
      "metadata": {
        "id": "35261a1b-2919-4b19-ac32-d2979a59ff07"
      },
      "outputs": [],
      "source": [
        "# Define LLM inference function to use later\n",
        "API_URL = \"https://inf.cl.uni-trier.de/chat/\"\n",
        "\n",
        "\n",
        "def llm(model_name, system_prompt, input_query, rag_context):\n",
        "    rag_context = f\"Additional Context: {rag_context}\"\n",
        "    # Construct the request payload\n",
        "    payload = {\n",
        "        \"messages\": [\n",
        "            {\"content\": system_prompt, \"role\": \"system\"},\n",
        "            {\"content\": rag_context, \"role\": \"system\"},\n",
        "            {\"content\": input_query, \"role\": \"user\"}\n",
        "        ],\n",
        "        \"model\": model_name,\n",
        "        \"options\": {}\n",
        "    }\n",
        "\n",
        "    # Set the request headers\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Send the POST request\n",
        "    response = requests.post(API_URL, headers=headers,\n",
        "                             data=json.dumps(payload))\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        print(\"Response received successfully:\")\n",
        "        response = response.json()  # json.dumps(, indent=4)\n",
        "    else:\n",
        "        print(\n",
        "            f\"Failed to retrieve response. Status code: {response.status_code}\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd18839-ffed-4b3b-beb3-9ccbe60a6cd1",
      "metadata": {
        "id": "8fd18839-ffed-4b3b-beb3-9ccbe60a6cd1"
      },
      "source": [
        "Here we make experiments with system prompt,\n",
        "\n",
        "System prompt contains : what model is supposed to do with the input query. with an exmple output.\n",
        "Input Query contains, one row of dataset.\n",
        "explaination variable will have the output of model which will be an explaination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eac2453b-a98f-4631-8cf8-cb46e49d5a38",
      "metadata": {
        "id": "eac2453b-a98f-4631-8cf8-cb46e49d5a38"
      },
      "outputs": [],
      "source": [
        "# # Testing LLM\n",
        "# system_prompt = \"You are a helpful assistant.\"\n",
        "# input_query = \"Hi\"\n",
        "# model_name = \"llama3.3:70b-instruct-q6_K\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a14604fb-0006-4238-b022-e20f502311de",
      "metadata": {
        "id": "a14604fb-0006-4238-b022-e20f502311de",
        "outputId": "223d1972-f328-418c-dc94-3ababa9f9695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response received successfully:\n",
            "Hello! How can I assist you today? Do you have any questions, need help with something, or just want to chat? I'm here to help!\n",
            "CPU times: user 99.4 ms, sys: 27.4 ms, total: 127 ms\n",
            "Wall time: 23 s\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "\n",
        "# response = llm(model_name, system_prompt, input_query)\n",
        "# explaination = response['response']\n",
        "# print(explaination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c0d230-e860-4dfb-bf05-13d3266a011f",
      "metadata": {
        "id": "d5c0d230-e860-4dfb-bf05-13d3266a011f"
      },
      "outputs": [],
      "source": [
        "# a[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49fe7b5d-c64e-41ee-af07-dbe6d2b77f56",
      "metadata": {
        "id": "49fe7b5d-c64e-41ee-af07-dbe6d2b77f56"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81de4bb-5d53-4c15-ae64-4c8269a426e0",
      "metadata": {
        "id": "b81de4bb-5d53-4c15-ae64-4c8269a426e0"
      },
      "source": [
        "## Load Embeddings model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36247ff3-4124-404e-a9df-765218c17d80",
      "metadata": {
        "id": "36247ff3-4124-404e-a9df-765218c17d80"
      },
      "source": [
        "If you are just experimenting with explainations and here to run LLM. then no need to run the cells in this section.\n",
        "\n",
        "Bellow there are 3 cells with 3 different type of models. uncomment the one suits best for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "de1d121c-87cb-4693-8f85-fdb530925264",
      "metadata": {
        "id": "de1d121c-87cb-4693-8f85-fdb530925264"
      },
      "outputs": [],
      "source": [
        "# RUNS superfast on CPU, Bad Results, good for old or weak laptop cpus, speed up testing\n",
        "\n",
        "# EMBEDDINGS_MODEL = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "580fdcdc-e66e-4a9b-841b-db8329eaed3e",
      "metadata": {
        "id": "580fdcdc-e66e-4a9b-841b-db8329eaed3e"
      },
      "outputs": [],
      "source": [
        "# # RUNS superfast on CPU too, Good Results, works well on laptop with good cpu and laptop without GPU.\n",
        "\n",
        "# EMBEDDINGS_MODEL = SentenceTransformer(\n",
        "#     'jxm/cde-small-v1', trust_remote_code=True, device='cuda').to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b6a38c7d-b7cc-4e08-a974-3b8bd11f180f",
      "metadata": {
        "id": "b6a38c7d-b7cc-4e08-a974-3b8bd11f180f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ece3514-3290-46ab-966c-dd9d8a18a3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "EMBEDDINGS_MODEL = SentenceTransformer(\n",
        "    \"dunzhang/stella_en_1.5B_v5\",\n",
        "    trust_remote_code=True,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "672bc77b-5526-4553-bc06-1408e7998099",
      "metadata": {
        "id": "672bc77b-5526-4553-bc06-1408e7998099",
        "outputId": "b7e6a998-34fe-4abc-c565-c3c141c9b11b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 538 ms, sys: 1.89 ms, total: 540 ms\n",
            "Wall time: 539 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# dimension\n",
        "len(EMBEDDINGS_MODEL.encode(\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb486448-c637-4fd3-8361-8a7a56557fcd",
      "metadata": {
        "id": "cb486448-c637-4fd3-8361-8a7a56557fcd"
      },
      "source": [
        "## Configure Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9882e102-03ef-463f-8eaf-ec6c1ace8687",
      "metadata": {
        "id": "9882e102-03ef-463f-8eaf-ec6c1ace8687"
      },
      "outputs": [],
      "source": [
        "# https://7ef18c4d-2ef6-4fb0-9243-0ac62546593c.us-east4-0.gcp.cloud.qdrant.io:6333/dashboard#/collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "00d88b83-d2c8-4e1d-949d-6532420dacb7",
      "metadata": {
        "id": "00d88b83-d2c8-4e1d-949d-6532420dacb7",
        "outputId": "346e23a3-deeb-4f99-c871-b8cc4c2a76d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collections=[CollectionDescription(name='HateXplain_8129'), CollectionDescription(name='HateXplain_index_2'), CollectionDescription(name='explainations_nimora_test'), CollectionDescription(name='HateXplain_index'), CollectionDescription(name='HateXplain_index_3'), CollectionDescription(name='HateXplain_gpu_stella_0'), CollectionDescription(name='HateXplain_gpu_nilo_0'), CollectionDescription(name='test_index'), CollectionDescription(name='HateXplain_index_1'), CollectionDescription(name='HateXplain_gpu_usama_0'), CollectionDescription(name='HateXplain_gpu_nilo_1'), CollectionDescription(name='test_index_'), CollectionDescription(name='HateXplain_index_4')]\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://7ef18c4d-2ef6-4fb0-9243-0ac62546593c.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"BR8zsNr5lEYrqJPL4EknUj2oRska2JO1nHwPFawlFMqZIrYMuGZ0Wg\",\n",
        ")\n",
        "\n",
        "print(qdrant_client.get_collections())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c0010b1b-ba3a-4f9d-bc24-5122cf237279",
      "metadata": {
        "id": "c0010b1b-ba3a-4f9d-bc24-5122cf237279"
      },
      "outputs": [],
      "source": [
        "# docker only\n",
        "# qdrant_client = QdrantClient(location='127.0.0.1', port=6333)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d95d3d7-46d9-45a3-bae1-de3a3283c8ee",
      "metadata": {
        "id": "2d95d3d7-46d9-45a3-bae1-de3a3283c8ee"
      },
      "source": [
        "You can browse the collections/indexes here:\n",
        "\n",
        "https://7ef18c4d-2ef6-4fb0-9243-0ac62546593c.us-east4-0.gcp.cloud.qdrant.io:6333/dashboard#/collections\n",
        "\n",
        "and and enter the API: BR8zsNr5lEYrqJPL4EknUj2oRska2JO1nHwPFawlFMqZIrYMuGZ0Wg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79d5fe5-ed18-4051-bd16-734267230613",
      "metadata": {
        "id": "a79d5fe5-ed18-4051-bd16-734267230613"
      },
      "source": [
        "## Deploy | Upload dataset on RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0c3c59fd-2546-4027-a3d9-0c123b8ddc0b",
      "metadata": {
        "id": "0c3c59fd-2546-4027-a3d9-0c123b8ddc0b"
      },
      "outputs": [],
      "source": [
        "index_name = \"HateXplain_gpu_stella_0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7121406-68f5-45a2-84f1-1c9bc964bdf2",
      "metadata": {
        "id": "d7121406-68f5-45a2-84f1-1c9bc964bdf2"
      },
      "source": [
        "## Query RAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_explainations_rag(user_input, top_k):\n",
        "\n",
        "    embedding = EMBEDDINGS_MODEL.encode(user_input)\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=index_name, query_vector=embedding, limit=top_k\n",
        "    )\n",
        "\n",
        "    result = [\n",
        "        {\n",
        "            \"text\": item.payload[\"tweet_text\"],\n",
        "            \"explanation\": item.payload[\"Response\"],\n",
        "            \"key_features\": item.payload[\"key_features\"],\n",
        "            # \"score\": item.score,\n",
        "            \"label\": item.payload[\"label\"]\n",
        "        }\n",
        "        for item in search_result\n",
        "    ]\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "FChE_GH-e6fF"
      },
      "id": "FChE_GH-e6fF",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFERENCE"
      ],
      "metadata": {
        "id": "cldWDj5vtQlb"
      },
      "id": "cldWDj5vtQlb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Userinput + RAG output - > LLM -> classification"
      ],
      "metadata": {
        "id": "-HSU_uBwm3fm"
      },
      "id": "-HSU_uBwm3fm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eak9xAaMm5-e"
      },
      "id": "Eak9xAaMm5-e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt_classification = \"\"\"\n",
        "Lets classify the following text for hate speech.\n",
        "\n",
        "if its hatespeech your reply will be: \"hate speech\"\n",
        "if its not hatespeech your reply will be : \"normal\"\n",
        "if its offensive and not hatespeech, your reply will be : \"offensive\"\n",
        "\n",
        "You are also given some related examples and their decision along with explainations why the decision was took. If those examples help make an unbiast decision contextually.\n",
        "Focus more on userinput.\n",
        "\n",
        "\n",
        "Here is user input and reply with only one word ONLY such as [hate speech, normal, or offensive] \"\"\""
      ],
      "metadata": {
        "id": "FgnQIPLAm6mP"
      },
      "id": "FgnQIPLAm6mP",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model_name = \"deepseek-r1:70b\"\n",
        "user_input = \"I might rape console players tonight\"\n",
        "top_k = 2  # number of explainations fetched from vector db\n",
        "\n",
        "\n",
        "user_input = \"Userinput:\" + user_input\n",
        "print(f\"######################################### \\nUserInput: {user_input}\")\n",
        "\n",
        "rag_results = fetch_explainations_rag(user_input, top_k)\n",
        "rag_results_str = json.dumps(rag_results, indent=4)\n",
        "\n",
        "print(f\"\\n######################################### \\nRAG Results:\")\n",
        "print(rag_results_str)\n",
        "\n",
        "response = llm(model_name, system_prompt_classification, user_input, rag_results_str)\n",
        "llm_classification_output = response['response']\n",
        "print(f\"\\n######################################### \\nLLM Classification output: \\n {llm_classification_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k805xo65nHUf",
        "outputId": "f1248404-1f85-412b-e4ce-8a5686d85261"
      },
      "id": "k805xo65nHUf",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################### \n",
            "UserInput: Userinput:I might rape console players tonight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-2f29db771a04>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  search_result = qdrant_client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################### \n",
            "RAG Results:\n",
            "[\n",
            "    {\n",
            "        \"text\": \"i ate dan because it raped me on minecraft\",\n",
            "        \"explanation\": \"The provided tweet text is classified as neutral due to the following reasons:\\n\\n1. The tweet's content does not contain any overt hate speech or directly offensive language.\\n\\n2. While references to sexual assault and violence are present (\\\"it raped me\\\"), they appear to be used in a non-literal context related to the game Minecraft, where \\\"rape\\\" can colloquially mean domination or defeat within the gaming community.\\n\\n3. The tweet does not target any specific group or individual, nor does it promote harm or discrimination against anyone.\\n\\n4. Given the lack of clear malicious intent and the absence of direct targeting or inflammatory language, this tweet is best categorized as neutral, despite its potentially disturbing content when taken out of context.\",\n",
            "        \"key_features\": \"[]\",\n",
            "        \"label\": \"normal\"\n",
            "    },\n",
            "    {\n",
            "        \"text\": \"<user> what if you get raped or the condom breaks\",\n",
            "        \"explanation\": \"The text falls into the 'Offensive' category for the following reasons:\\n\\n1. The tweet directly targets another user with a sexually violent and threatening remark.\\n2. Key features include the words 'raped' and 'condom breaks', which carry explicit sexual connotations and imply a threat of non-consensual sex or pregnancy. These phrases contribute to the hostile tone and intent of the message.\\n3. The target is another user who is directly addressed in a demeaning and aggressive manner. This personalizes the attack, making it more severe and intimidating.\\n4. The tweet's tone is confrontational and menacing, using hypothetical violence to belittle or threaten the recipient. It creates an unsettling context that promotes harm.\\n5. In conclusion, the text is classified as 'Offensive' due to its sexually explicit threat of violence against a targeted individual. The aggressive language and implied assault make it a clear case of online harassment and intimidation.\",\n",
            "        \"key_features\": \"[]\",\n",
            "        \"label\": \"normal\"\n",
            "    }\n",
            "]\n",
            "Response received successfully:\n",
            "\n",
            "######################################### \n",
            "LLM Classification output: \n",
            " <think>\n",
            "Okay, let's see. I have to classify the user input as hate speech, normal, or offensive. The input is: \"I might rape console players tonight.\"\n",
            "\n",
            "First, I should consider the context and the words used. The word \"rape\" is a strong term that typically refers to sexual assault, which is a serious issue. However, in gaming contexts, especially among players, terms like \"rape\" can sometimes be slang for dominating or defeating someone in the game.\n",
            "\n",
            "So, in this case, the user says they might \"rape console players.\" Console players are people who play video games on consoles like PlayStation or Xbox. The phrase could imply that the user is going to dominate them in a game, not literally commit any violence. \n",
            "\n",
            "But I need to be careful here because even if it's slang, using the word \"rape\" can still be offensive and triggering for some people. It's a sensitive term, so even if the intent is about gaming, the language itself might cross into being offensive rather than just normal or hateful.\n",
            "\n",
            "Looking at the examples provided earlier, in one case, the explanation mentioned that references to rape were in a non-literal context related to Minecraft, and it was classified as \"normal.\" However, another example where someone was directly targeted with a threatening remark about rape was classified as \"offensive.\"\n",
            "\n",
            "In this current input, the user is talking about console players in general, not targeting a specific individual. So maybe it's more offensive rather than hate speech because it uses a violent term but doesn't target a specific group beyond gamers.\n",
            "\n",
            "Hate speech usually involves attacking a person or group based on characteristics like race, religion, etc., with intent to harm or incite violence. Here, the term is used in a gaming context, so it might not meet the criteria for hate speech but could still be offensive because of the word choice.\n",
            "\n",
            "So, considering all that, I think the correct classification here would be \"offensive.\"\n",
            "</think>\n",
            "\n",
            "offensive\n",
            "CPU times: user 2.3 s, sys: 3.35 s, total: 5.65 s\n",
            "Wall time: 50.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## userinput + classification  -> Explaination or decision"
      ],
      "metadata": {
        "id": "AqO2uf5zqtuQ"
      },
      "id": "AqO2uf5zqtuQ"
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt_explaination = \"\"\"\n",
        "You are a helpful chatbot, which acts normal when the label is normal, but when you get userinput along with label as offensive or hatespeech. you have to tell the user why its hatespeech or offensive.\n",
        "And tell the user what he should tell, help him learn and avoid\n",
        "\n",
        "You are given userinput, and the label bellow: \"\"\""
      ],
      "metadata": {
        "id": "vKvgMuzfrErg"
      },
      "id": "vKvgMuzfrErg",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model_name = \"deepseek-r1:70b\"\n",
        "user_input = \"I might rape console players tonight\"\n",
        "top_k = 2  # number of explainations fetched from vector db\n",
        "\n",
        "\n",
        "user_input = \"Userinput:\" + user_input\n",
        "print(f\"######################################### \\nUserInput: {user_input}\")\n",
        "\n",
        "classified_label = \"offensive\"\n",
        "response = llm(model_name, system_prompt_explaination, user_input, classified_label)\n",
        "llm_output = response['response']\n",
        "print(f\"\\n######################################### \\nLLM final output for user: \\n {llm_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83H8fCC2rAmx",
        "outputId": "13d97778-3daa-440c-c2b8-3f579fb99326"
      },
      "id": "83H8fCC2rAmx",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################### \n",
            "UserInput: Userinput:I might rape console players tonight\n",
            "Response received successfully:\n",
            "\n",
            "######################################### \n",
            "LLM final output for user: \n",
            " <think>\n",
            "Okay, I need to figure out how to respond to the user who said, \"I might rape console players tonight.\" The label is offensive, so my role is to explain why this is problematic and help the user understand what they should say instead.\n",
            "\n",
            "First, I'll break down the issues with their statement. Using the word \"rape\" in this context is a violent term that can cause distress. Even though it might be meant as a joke or exaggeration, it's not appropriate because it trivializes sexual assault. Also, targeting console players specifically can make them feel attacked just for their gaming preferences.\n",
            "\n",
            "I should explain why this language is offensive. It's important to mention how using such terms can be hurtful and promote a negative environment. Then, I'll offer alternative ways they could express their competitive spirit without offending others. For example, suggesting phrases that focus on the game itself rather than attacking others.\n",
            "\n",
            "I need to make sure my response is clear and helpful, encouraging positive communication instead of harmful language.\n",
            "</think>\n",
            "\n",
            "The statement you made can be hurtful because it uses violent imagery that trivializes serious issues like sexual assault. It's important to express competitiveness in a way that doesn't target or offend others. Consider focusing on the game itself with phrases like, \"I'm ready to challenge console players tonight!\" This approach fosters a positive and inclusive environment for everyone involved.\n",
            "CPU times: user 143 ms, sys: 18.9 ms, total: 162 ms\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCdVZmKysKUX"
      },
      "id": "hCdVZmKysKUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiXmaecOsx92"
      },
      "id": "oiXmaecOsx92",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "07c06ab9-fd82-401c-a396-40020d9ab9f1",
        "b81de4bb-5d53-4c15-ae64-4c8269a426e0",
        "cb486448-c637-4fd3-8361-8a7a56557fcd",
        "d7121406-68f5-45a2-84f1-1c9bc964bdf2",
        "-HSU_uBwm3fm",
        "AqO2uf5zqtuQ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}